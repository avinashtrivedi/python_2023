{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking/Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking (sometimes called stacked generalization or blending) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. In practice, a single-layer logistic regression model is often used as the combiner, although stacking can theoretically represent a variety of ensemble techniques by using any arbitrary combiner algorithm. Stacking/blending typically yields performance better than any single one of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (c) 2014 Reid Johnson\n",
    "#\n",
    "# Modified from:\n",
    "# Kemal Eren (https://github.com/kemaleren/scikit-learn/blob/stacking/sklearn/ensemble/stacking.py)\n",
    "#\n",
    "# Generates a stacking/blending of base models. Cross-validation is used to \n",
    "# generate predictions from base (level-0) models that are used as input to a \n",
    "# combiner (level-1) model.\n",
    "\n",
    "import numpy as np\n",
    "#from itertools import zip\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "#from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin\n",
    "from sklearn.ensemble.base import BaseEnsemble\n",
    "from sklearn.utils.validation import assert_all_finite\n",
    "\n",
    "# TODO: Built-in nested cross validation, re-using base classifiers, to pick \n",
    "#       best stacking method.\n",
    "# TODO: Access to best, vote, etc. after training.\n",
    "\n",
    "__all__ = [\n",
    "    \"Stacking\",\n",
    "    \"StackingFWL\",\n",
    "    'estimator_grid'\n",
    "]\n",
    "\n",
    "\n",
    "def estimator_grid(*args):\n",
    "    \"\"\"Generate candidate estimators from a list of parameter values on the \n",
    "    combination of the various parameter lists given.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : array\n",
    "        List of classifiers and corresponding parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array\n",
    "        The generated estimators.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    pairs = zip(args[::2], args[1::2])\n",
    "    for estimator, params in pairs:\n",
    "        if len(params) == 0:\n",
    "            result.append(estimator())\n",
    "        else:\n",
    "            for p in ParameterGrid(params):\n",
    "                result.append(estimator(**p))\n",
    "    return result\n",
    "\n",
    "\n",
    "class MRLR(ClassifierMixin):\n",
    "    \"\"\"Converts a multi-class classification task into a set of indicator \n",
    "    regression tasks.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] K. M. Ting, I. H. Witten, \"Issues in Stacked Generalization\", 1999.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, regressor, stackingc, **kwargs):\n",
    "        self.estimator_ = regressor\n",
    "        self.estimator_args_ = kwargs\n",
    "        self.stackingc_ = stackingc\n",
    "\n",
    "    def _get_subdata(self, X):\n",
    "        \"\"\"Returns subsets of the data, one for each class. Assumes the \n",
    "        columns of X are striped in order.\n",
    "\n",
    "        e.g. if n_classes_ == 3, then returns (X[:, 0::3], X[:, 1::3],\n",
    "        X[:, 2::3])\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array of shape = [len(set(y)), n_samples]\n",
    "            The subsets of the data.\n",
    "        \"\"\"\n",
    "        if not self.stackingc_:\n",
    "            return [X, ] * self.n_classes_\n",
    "\n",
    "        result = []\n",
    "        for i in range(self.n_classes_):\n",
    "            slc = (slice(None), slice(i, None, self.n_classes_))\n",
    "            result.append(X[slc])\n",
    "        return result\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the estimator given predictor(s) X and target y. Assumes the\n",
    "        columns of X are predictions generated by each predictor on each \n",
    "        class. Fits one estimator for each class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output.\n",
    "\n",
    "        y : array of shape = [n_samples]\n",
    "            The actual outputs (class data).\n",
    "        \"\"\"\n",
    "        self.n_classes_ = len(set(y))\n",
    "        self.estimators_ = []\n",
    "\n",
    "        # Generate feature data subsets corresponding to each class.\n",
    "        X_subs = self._get_subdata(X)\n",
    "\n",
    "        # Fit an instance of the estimator to each data subset.\n",
    "        for i in range(self.n_classes_):\n",
    "            e = self.estimator_(**self.estimator_args_)\n",
    "            y_i = np.array(list(j == i for j in y))\n",
    "            X_i = X_subs[i]\n",
    "            e.fit(X_i, y_i)\n",
    "            self.estimators_.append(e)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict label values with the fitted estimator on predictor(s) X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array of shape = [n_samples]\n",
    "            The predicted label values of the input samples.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict label probabilities with the fitted estimator on \n",
    "        predictor(s) X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba : array of shape = [n_samples]\n",
    "            The predicted label probabilities of the input samples.\n",
    "        \"\"\"\n",
    "        proba = []\n",
    "\n",
    "        X_subs = self._get_subdata(X)\n",
    "\n",
    "        for i in range(self.n_classes_):\n",
    "            e = self.estimators_[i]\n",
    "            X_i = X_subs[i]\n",
    "            pred = e.predict(X_i).reshape(-1, 1)\n",
    "            proba.append(pred)\n",
    "        proba = np.hstack(proba)\n",
    "\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        assert_all_finite(proba)\n",
    "\n",
    "        return proba\n",
    "\n",
    "\n",
    "class Stacking(BaseEnsemble):\n",
    "    \"\"\"Implements stacking/blending.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    meta_estimator : string or callable\n",
    "        May be one of \"best\", \"vote\", \"average\", or any classifier or \n",
    "        regressor constructor\n",
    "\n",
    "    estimators : iterator\n",
    "        An iterable of estimators; each must support predict_proba()\n",
    "\n",
    "    cv : iterator\n",
    "        A cross validation object. Base (level-0) estimators are trained on \n",
    "        the training folds, then the meta (level-1) estimator is trained on \n",
    "        the testing folds.\n",
    "\n",
    "    stackingc : bool\n",
    "        Whether to use StackingC or not. For more information, refer to the \n",
    "        following paper:\n",
    "\n",
    "        Reference:\n",
    "          A. K. Seewald, \"How to Make Stacking Better and Faster While Also \n",
    "          Taking Care of an Unknown Weakness,\" 2002.\n",
    "\n",
    "    kwargs :\n",
    "        Arguments passed to instantiate meta_estimator.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] D. H. Wolpert, \"Stacked Generalization\", 1992.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Support different features for each estimator.\n",
    "    # TODO: Support \"best\", \"vote\", and \"average\" for already trained model.\n",
    "    # TODO: Allow saving of estimators, so they need not be retrained when \n",
    "    #       trying new stacking methods.\n",
    "\n",
    "    def __init__(self, meta_estimator, estimators,\n",
    "                 cv, stackingc=True, proba=True,\n",
    "                 **kwargs):\n",
    "        self.estimators_ = estimators\n",
    "        self.n_estimators_ = len(estimators)\n",
    "        self.cv_ = cv\n",
    "        self.stackingc_ = stackingc\n",
    "        self.proba_ = proba\n",
    "\n",
    "        if stackingc:\n",
    "            if isinstance(meta_estimator, str) or not issubclass(meta_estimator, RegressorMixin):\n",
    "                raise Exception('StackingC only works with a regressor.')\n",
    "\n",
    "        if isinstance(meta_estimator, str):\n",
    "            if meta_estimator not in ('best',\n",
    "                                      'average',\n",
    "                                      'vote'):\n",
    "                raise Exception('Invalid meta estimator: {0}'.format(meta_estimator))\n",
    "            raise Exception('\"{0}\" meta estimator not implemented'.format(meta_estimator))\n",
    "        elif issubclass(meta_estimator, ClassifierMixin):\n",
    "            self.meta_estimator_ = meta_estimator(**kwargs)\n",
    "        elif issubclass(meta_estimator, RegressorMixin):\n",
    "            self.meta_estimator_ = MRLR(meta_estimator, stackingc, **kwargs)\n",
    "        else:\n",
    "            raise Exception('Invalid meta estimator: {0}'.format(meta_estimator))\n",
    "\n",
    "    def _base_estimator_predict(self, e, X):\n",
    "        \"\"\"Predict label values with the specified estimator on predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        e : int\n",
    "            The estimator object.\n",
    "\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred : np.ndarray, shape=(len(X), 1)\n",
    "            The mean of the label probabilities predicted by the specified \n",
    "            estimator for each fold for each instance X.\n",
    "        \"\"\"\n",
    "        # Generate array for the base-level testing set, which is n x n_folds.\n",
    "        pred = e.predict(X)\n",
    "        assert_all_finite(pred)\n",
    "        return pred\n",
    "\n",
    "    def _base_estimator_predict_proba(self, e, X):\n",
    "        \"\"\"Predict label probabilities with the specified estimator on \n",
    "        predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        e : int\n",
    "            The estimator object.\n",
    "\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted outputs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred : np.ndarray, shape=(len(X), 1)\n",
    "            The mean of the label probabilities predicted by the specified \n",
    "            estimator for each fold for each instance X.\n",
    "        \"\"\"\n",
    "        # Generate array for the base-level testing set, which is n x n_folds.\n",
    "        pred = e.predict_proba(X)\n",
    "        assert_all_finite(pred)\n",
    "        return pred\n",
    "\n",
    "    def _make_meta(self, X):\n",
    "        \"\"\"Make the feature set for the meta (level-1) estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An n x len(self.estimators_) array of meta-level features.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for e in self.estimators_:\n",
    "            if self.proba_:\n",
    "                # Predict label probabilities\n",
    "                pred = self._base_estimator_predict_proba(e, X)\n",
    "            else:\n",
    "                # Predict label values\n",
    "                pred = self._base_estimator_predict(e, X)\n",
    "            rows.append(pred)\n",
    "        return np.hstack(rows)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the estimator given predictor(s) X and target y.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data on which to fit.\n",
    "\n",
    "        y : array of shape = [n_samples]\n",
    "            The actual outputs (class data).\n",
    "        \"\"\"\n",
    "        # Build meta data.\n",
    "        X_meta = [] # meta-level features\n",
    "        y_meta = [] # meta-level labels\n",
    "\n",
    "        print ('Training and validating the base (level-0) estimator(s)...')\n",
    "        print()\n",
    "        for i, (a, b) in enumerate(self.cv_.split(X, y)):\n",
    "            print ('Fold [%s]' % (i))\n",
    "\n",
    "            X_a, X_b = X[a], X[b] # training and validation features\n",
    "            y_a, y_b = y[a], y[b] # training and validation labels\n",
    "\n",
    "            # Fit each base estimator using the training set for the fold.\n",
    "            for j, e in enumerate(self.estimators_):\n",
    "                print ('  Training base (level-0) estimator %d...' % (j)),\n",
    "                e.fit(X_a, y_a)\n",
    "                print ('done.')\n",
    "\n",
    "            proba = self._make_meta(X_b)\n",
    "            X_meta.append(proba)\n",
    "            y_meta.append(y_b)\n",
    "        print\n",
    "\n",
    "        X_meta = np.vstack(X_meta)\n",
    "        if y_meta[0].ndim == 1:\n",
    "            y_meta = np.hstack(y_meta)\n",
    "        else:\n",
    "            y_meta = np.vstack(y_meta)\n",
    "\n",
    "        # Train meta estimator.\n",
    "        print ('Training meta (level-1) estimator...'),\n",
    "        self.meta_estimator_.fit(X_meta, y_meta)\n",
    "        print ('done.')\n",
    "\n",
    "        # Re-train base estimators on full data.\n",
    "        for j, e in enumerate(self.estimators_):\n",
    "            print ('Re-training base (level-0) estimator %d on full data...' % (j)),\n",
    "            e.fit(X, y)\n",
    "            print ('done.')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict label values with the fitted estimator on predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array of shape = [n_samples]\n",
    "            The predicted label values of the input samples.\n",
    "        \"\"\"\n",
    "        X_meta = self._make_meta(X)\n",
    "        return self.meta_estimator_.predict(X_meta)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict label probabilities with the fitted estimator on \n",
    "        predictor(s) X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape=(n, m)\n",
    "            The feature data for which to compute the predicted output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array of shape = [n_samples]\n",
    "            The predicted label probabilities of the input samples.\n",
    "        \"\"\"\n",
    "        X_meta = self._make_meta(X)\n",
    "        return self.meta_estimator_.predict_proba(X_meta)\n",
    "\n",
    "\n",
    "class StackingFWL(Stacking):\n",
    "    \"\"\"Implements Feature-Weighted Linear Stacking.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Sill, G. Takács, L. Mackey, D. Lin, \"Feature-Weighted Linear \n",
    "           Stacking\", 2009.\n",
    "\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will demonstrate stacking/blending by using the Iris flower dataset. Thus, we first load and perform some preprocessing on the data. The preprocessing involves altering the target or class variables, which in the Iris dataset are by default represented as strings (nominal values), but for compatibility reasons need to be represented as integers (numeric values). We perform this conversion using a label-encoding method available via scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_encode = True\n",
    "\n",
    "# Load the Iris flower dataset\n",
    "fileURL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "iris = pd.read_csv(fileURL, \n",
    "                   names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'],\n",
    "                   header=None)\n",
    "iris = iris.dropna()\n",
    "\n",
    "X = iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']] # features\n",
    "labels = iris['Species'] # class\n",
    "\n",
    "if label_encode:\n",
    "    # Transform string (nominal) output to numeric\n",
    "    y = preprocessing.LabelEncoder().fit_transform(labels)\n",
    "else:\n",
    "    y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate the base (level-0) models, which are the models whose predictions on the training data will be combined by a higher-level (level-1) model. Here, we use different variants of decision trees as our base models, specifically random forest and extra trees, both of which are decision tree ensembles (collections of individual decision trees). By default, we use 10 trees for each ensemble. We generate an array of these base models, which will be used as input to our stacking algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "n_trees = 10\n",
    "\n",
    "# Generate a list of base (level 0) classifiers.\n",
    "clfs = [RandomForestClassifier(n_estimators=n_trees, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=n_trees, n_jobs=-1, criterion='entropy'),\n",
    "        #GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=n_trees)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we partition the dataset into non-overlapping training and testing sets, with 60% of the data allocated to the training set and 40% allocated to the testing set. All of the training for the stacking algorithm will be performed on the training set, while the testing set will be used solely to generate predictions, the accuracy of which we will later evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The training sets will be used for all training and validation purposes.\n",
    "# The testing sets will only be used for evaluating the final blended (level 1) classifier.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of stacking requires that the base models generate output that can be further processed by a higher-level model. To generate this output, the base models must produce predictions on some sort of testing data. The base models cannot use the original testing set, as this data should not be used to influence model training, while the predictions generated by the base models will be used to train the higher-level model. Thus, the training set must itself be divided into training and testing portions for the base models, which can be accomplished by cross-validation.\n",
    "\n",
    "Here, we use 5-fold cross-validation to partition the training set into five non-overlapping sets or folds. Note that the folds we generate are stratified, which means that each fold contains roughly the same proportion of each class label. The output is a cross-validation object, which  has the information of which instances belong to which folds. This will be used as an input to our stacking algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Generate k stratified folds of the training data.\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our stacking algorithm will iterate through the training set folds previously generated. At each iteration, the selected fold will be used for validation, while the remaining folds will be used for training. The predictions generated by the base models on the validation set will be used as training features or predictor variables input to the higher-level model, while the predictions generated by the base models on the original testing set will be used as testing features or predictor variables input to the higher-level model. The labels (the class or target variable) will remain the same. Thus, one may think of this process as replacing the original feature values for each instance by the predictions made by each model. By using cross-validation, we are able to use the original training portion of the dataset to both train and evaluate our base models, which allows us to obtain predictions over (and thus generate new feature values for) all of the training instances. Since our higher-level model will be trained on these new feature values, we must also replace the original feature values for the testing data with the base model predictions on the testing data. As these preditions can be generated over the entire testing set on each fold, the average predictions over all folds will be used as testing input to the higher-level model.\n",
    "\n",
    "Once the base models are trained, they are stacked/blended. This means that their outputs (predictions) are used as input to a higher-level (level-1) model. Here, we use logistic regression as the higher-level model. As a result, the output generated by the logistic regression model, which is trained or fit on the predictions of the lower-level models, is used to predict the target or class variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating the base (level-0) estimator(s)...\n",
      "\n",
      "Fold [0]\n",
      "  Training base (level-0) estimator 0...\n",
      "done.\n",
      "  Training base (level-0) estimator 1...\n",
      "done.\n",
      "Fold [1]\n",
      "  Training base (level-0) estimator 0...\n",
      "done.\n",
      "  Training base (level-0) estimator 1...\n",
      "done.\n",
      "Fold [2]\n",
      "  Training base (level-0) estimator 0...\n",
      "done.\n",
      "  Training base (level-0) estimator 1...\n",
      "done.\n",
      "Fold [3]\n",
      "  Training base (level-0) estimator 0...\n",
      "done.\n",
      "  Training base (level-0) estimator 1...\n",
      "done.\n",
      "Fold [4]\n",
      "  Training base (level-0) estimator 0...\n",
      "done.\n",
      "  Training base (level-0) estimator 1...\n",
      "done.\n",
      "Training meta (level-1) estimator...\n",
      "done.\n",
      "Re-training base (level-0) estimator 0 on full data...\n",
      "done.\n",
      "Re-training base (level-0) estimator 1 on full data...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "stk = Stacking(LogisticRegression, clfs, skf, stackingc=False, proba=True)\n",
    "stk.fit(X_train.values, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the performance of the ensemble of blended models to that of the individual (base) ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blended Classifier Accuracy = 0.933333333333\n",
      "\n",
      "Random Forest (10 trees) Accuracy = 0.916666666667\n",
      "Extra Trees (10 trees) Accuracy = 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "### Generate predictions with stacked/blended (level-1) classifier. ###\n",
    "\n",
    "score = metrics.accuracy_score(y_test, stk.predict(X_test))\n",
    "print()\n",
    "print( 'Blended Classifier Accuracy = %s' % (score))\n",
    "print()\n",
    "\n",
    "### Generate predictions with base (level-0) classifiers. ###\n",
    "\n",
    "# Random forest predictions.\n",
    "score0 = metrics.accuracy_score(y_test, stk._base_estimator_predict(stk.estimators_[0], X_test))\n",
    "print( 'Random Forest (10 trees) Accuracy = %s' % (score0))\n",
    "\n",
    "# Extra trees predictions.\n",
    "score1 = metrics.accuracy_score(y_test, stk._base_estimator_predict(stk.estimators_[1], X_test))\n",
    "print( 'Extra Trees (10 trees) Accuracy = %s' % (score1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that by using the method of stacking/blending, we generate a \"meta\" model that will often outperform each of the base models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
