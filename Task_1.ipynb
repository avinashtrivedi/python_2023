{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2E5kRk-evfk"
   },
   "source": [
    "# Using pre-trained word embeddings with CNN\n",
    "\n",
    "This code is taken from KERAS.IO\n",
    "\n",
    "The original file:\n",
    "https://keras.io/examples/nlp/pretrained_word_embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW2gDOveevfp"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zqMCtKB_evfp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTWGhrSdevfr"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we show how to train a text classification model that uses pre-trained\n",
    "word embeddings.\n",
    "\n",
    "We'll work with the Newsgroup20 dataset, a set of 20,000 message board messages\n",
    "belonging to 20 different topic categories.\n",
    "\n",
    "For the pre-trained word embeddings, we'll use\n",
    "[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzSyJ7mJevfr"
   },
   "source": [
    "## Download the Newsgroup20 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cKYTKxdevfs",
    "outputId": "58e19fb1-8ef3-4155-807b-3e3100f23f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
      "17329808/17329808 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr1JGx2uevfs"
   },
   "source": [
    "## Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXxnlGHbevft",
    "outputId": "bf2a7134-5c44-4e9f-ad50-6e83328bddc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['comp.graphics', 'rec.autos', 'talk.politics.mideast', 'talk.politics.guns', 'alt.atheism', 'soc.religion.christian', 'rec.motorcycles', 'talk.politics.misc', 'rec.sport.baseball', 'rec.sport.hockey', 'misc.forsale', 'sci.crypt', 'sci.electronics', 'sci.space', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.os.ms-windows.misc', 'talk.religion.misc', 'comp.windows.x', 'sci.med']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['38530', '38649', '38616', '38626', '38571']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Plk0RtXLevft"
   },
   "source": [
    "Here's a example of what one file contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vE7dBZ6evfu",
    "outputId": "9ea2c229-1a99-4509-b8bb-1d6d0c15cdb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7THgWzMevfu"
   },
   "source": [
    "As you can see, there are header lines that are leaking the file's category, either\n",
    "explicitly (the first line is literally the category name), or implicitly, e.g. via the\n",
    "`Organization` filed. Let's get rid of the headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKcApOQRevfv",
    "outputId": "b6a7eaa9-020f-4c58-ac6d-c3984ac56ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3CySSBZevfv"
   },
   "source": [
    "There's actually one category that doesn't have the expected number of files, but the\n",
    "difference is small enough that the problem remains a balanced classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95TgXjVX-Wm-",
    "outputId": "cfe5fec3-ba5d-4702-ee45-6bc8b0706d4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 19997)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN2KFw5aevfv"
   },
   "source": [
    "## Shuffle and split the data into training & validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "u1zjPMQ7evfw"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCBaVgm1b_M0",
    "outputId": "90d0bf69-36bc-4775-cd70-66dc28655d22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nIn article <1993Apr21.040956.12823@wuecl.wustl.edu>\\nmvs1@cec2.wustl.edu (Michael Virata Sy) writes:\\n \\n>\\n>        Don't forget Paul Ysebaert, ex-Devil.  He's a good team player.\\n \\n      And Dino Ciccarelli and Ray Sheppard and so on and so on.....\\n \\n    : )\\n \\nLaurie Marshall\\nWayne State University\\nDetroit, Michigan\\nGo Wings!!!!\\n\",\n",
       " 'Approved: christian@aramis.rutgers.edu\\n\\nIn article <Apr.21.03.25.41.1993.1322@geneva.rutgers.edu>  \\nJBUDDENBERG@vax.cns.muskingum.edu (Jimmy Buddenberg) writes:\\n> \\n> Hello all.  We are doing a bible study (at my college) on Revelations.  We\\n> have been doing pretty good as far as getting some sort of reasonable\\n> interpretation.  We are now on chapters 17 and 18 which talk about the\\n> woman on the beast and the fall of Babylon.  I believe the beast is the\\n> Antichrist (some may differ but it seems obvious) and the woman represents\\n> Babylon which stands for Rome or the Roman Catholic Church.  What are some\\n> views on this interpretation?  Is the falling Babylon in chapter 18 the same\\n> Babylon in as in chapter 17?  The Catholic church?\\n> Hate to step on toes.\\n> thanks\\n\\nAn interesting interpretation of Revelation 17 and 18 has been given by  \\nevangelist David Wilkerson.  I am not saying that I totally agree with his  \\ninterpretation, but it is certainly believable and good food for thought.  He  \\ninterprets the Babylon of Revelation 17-18 as being none other than the good  \\nold U. S. of A.  That\\'s right, America.  He supports his claim in several ways.   \\nThe Babylon of Revelation is THE world leader in trade and commerce, and the  \\nWHOLE WORLD wept when Babylon fell.  The American dollar, despite the Japanese  \\nsuccess of the 20th century, is STILL the most sought after currency in the  \\nworld.  If the U.S. were destroyed, wouldn\\'t the whole world mourn?  The bible  \\nalso talks about Babylon being a home of harlots, sin, and adultery (I am  \\nparaphrasing, of course).  Babylon\\'s sin affected, or should I say, infected,  \\nthe whole world.  It doesn\\'t take much looking to see that the U.S. is in a  \\nstate of moral decay.  Hasn\\'t the American culture and Hollywood spread the \"do  \\nit if it feels good\" mentality all over the world.  I think, though, that what  \\nMr. Wilkerson uses as his strongest argument is the fact that Revelation calls  \\nBabylon \"Babylon the Great\" and portrays it as the most powerful nation on  \\nearth.  No matter how dissatisfied you are with the state of our country, I  \\ndon\\'t think you would have too much trouble agreeing that the U.S. is STILL the  \\nmost powerful nation on earth.\\n\\nAgain, this interpretation is not NECESSARILY my own, but I do find it worthy  \\nof consideration.\\n\\nJeffrey Little\\n',\n",
       " \"NNTP-Posting-Host: stein2.u.washington.edu\\n\\ndemon@desire.wright.edu (Not a Boomer) writes:\\n>\\tA judge denied GM's new trial motion, even though GM says it has two\\n>new witnesses that said the occupant of the truck was dead from the impact, not\\n>from the fire.\\n>\\n>\\tThoughts?\\n\\nHow can a witness tell that someone in a burning truck is dead rather than\\nunconscious?\\n\\n>\\tIt's kind of scary when you realize that judges are going to start\\n>denying new trials even when new evidence that contradicts the facts that led\\n>to the previous ruling appear.\\n>\\n>\\tOr has the judge decided that the new witnesses are not to be believed? \\n>Shouldn't that be up to a jury?\\n\\nWhat kind of witnesses?  If we are talking about witnesses who were at\\nthe accident, or were otherwise directly involved (e.g., paramedics,\\nemergency room doctors, etc.), then they should have been used at the\\nfirst trial.  You don't get a new trial because you screwed up and\\nforgot to call all of your witnesses.\\n\\nIf we are talking about new expert witnesses who will offer new\\ninterpretations of the data, note that the loser can *ALWAYS* find\\nsuch witnesses.  If this were grounds for a new trial, then the loser\\ncould *ALWAYS* get a new trial, and keep doing so until the loser\\nbecomes a winner (and then the other side would come up with new\\nexpert witnesses).\\n\\n--Tim Smith\\n\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzsBsQuIevfw"
   },
   "source": [
    "## Create a vocabulary index\n",
    "\n",
    "Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n",
    "Later, we'll use the same layer instance to vectorize the samples.\n",
    "\n",
    "Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n",
    "be actually 200 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KH9ZkV6Kevfw"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_8tXL-aevfx"
   },
   "source": [
    "You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n",
    "print the top 5 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MawEWABTevfx",
    "outputId": "fca22270-b600-4d13-f116-b93a751bedf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " 'and',\n",
       " 'in',\n",
       " 'is',\n",
       " 'i',\n",
       " 'that',\n",
       " 'it',\n",
       " 'for',\n",
       " 'you',\n",
       " 'this',\n",
       " 'on']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cD_DchIMZMu7",
    "outputId": "b30cfcf4-9203-4bee-9a59-b82ec4dcce44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of voc\n",
    "len(vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKsV2c20evfx"
   },
   "source": [
    "Let's vectorize a test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dffpv5Xxevfy",
    "outputId": "06cb6633-9651-4b27-b217-22f062f8d96d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3596, 1735,   15,    2, 6994])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vectorizer([[\"the cat sat on the mat\"]] )\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_BGqQuPZTIj",
    "outputId": "a6729f09-a21c-409a-fca3-178adee99350"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3596, 1735,   15,    2, 6994,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.numpy()[0, :16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzbYvBclevfy"
   },
   "source": [
    "As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n",
    "word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n",
    "reserved for \"out of vocabulary\" tokens.\n",
    "\n",
    "Here's a dict mapping words to their indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_AdOTYNfevfy"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkwPFOOCevfy"
   },
   "source": [
    "As you can see, we obtain the same encoding as above for our test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2rHmNyvevfz",
    "outputId": "0ea1e404-c63d-4ffa-b722-2b581cd667a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3596, 1735, 15, 2, 6994]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E62nJCUdevfz"
   },
   "source": [
    "## Load pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3TqqCtuevfz"
   },
   "source": [
    "Let's download pre-trained GloVe embeddings (a 822M zip file).\n",
    "\n",
    "You'll need to run the following commands:\n",
    "\n",
    "```\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHJ6lrivevfz"
   },
   "source": [
    "The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
    "100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
    "\n",
    "Let's make a dict mapping words (strings) to their NumPy vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMXczRLovkL9",
    "outputId": "db13d1b6-15a5-488e-ba75-c3973cc50f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-23 01:04:41--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-03-23 01:04:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-03-23 01:04:42--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
      "\n",
      "2023-03-23 01:07:21 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRsbqbpGainz",
    "outputId": "740d31f5-51f3-4011-860f-7d2c482e0aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config',\n",
       " 'glove.6B.200d.txt',\n",
       " 'glove.6B.300d.txt',\n",
       " 'glove.6B.100d.txt',\n",
       " 'glove.6B.zip',\n",
       " 'glove.6B.50d.txt',\n",
       " 'sample_data']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bra03s-payJ6",
    "outputId": "bc34aabe-b810-4092-b916-d8513b53e79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "#!head -10 glove.6B.50d.txt\n",
    "!wc -l glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpWeop-Fevfz",
    "outputId": "4b2b495a-8701-4e3c-d337-b532561e7e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file =\"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dt8jCODha81G",
    "outputId": "12a94397-7fc4-48c1-ffcd-8a78e68dfba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOt5ivHCevf0"
   },
   "source": [
    "Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
    "`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
    "vector for the word of index `i` in our `vectorizer`'s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgNQ00_Fevf0",
    "outputId": "6f87130b-e8f9-4c8a-bf20-4dda65823302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17956 words (2044 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyePhjtGevf0"
   },
   "source": [
    "Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n",
    "\n",
    "Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n",
    "update them during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "M7lcI_Scevf0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue77DyMyevf0"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "A simple 1D convnet with global max pooling and a classifier at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxlOiVI8evf1",
    "outputId": "df03c98a-f243-47bc-efb7-d7d8a3f0f813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         2000200   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,247,516\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB_9OYtvevf1"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n",
    "are right-padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "z_ilfygwevf1"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9K-1cXkcSVn",
    "outputId": "5627d28e-6f42-48ae-f180-410b7b887b46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15998, 200), (15998,), (3999, 200), (3999,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEzqvQaSevf1"
   },
   "source": [
    "We use categorical crossentropy as our loss since we're doing softmax classification.\n",
    "Moreover, we use `sparse_categorical_crossentropy` since our labels are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7uLQuWncnOK",
    "outputId": "27ba1750-9def-4b1a-c1ce-338aad479d9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 15, 18,  5, 15])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQOZQYvNDaPS",
    "outputId": "d44b45b5-beec-4445-863c-c12d3dd32694"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1349,  3581,   418,  4569,    75,   255,     7,     1,     1,\n",
       "          40,   710,  3809,     9,    96,   332,    12,     2,   271,\n",
       "           4,  5191,     7,     1,    54,    46,  1939,   705,     9,\n",
       "          96,  1372,     3,  1496,     5,   187,    12,   370,     7,\n",
       "           1,    41,  2724,     1,     9,    96,   395,     7,  6476,\n",
       "        2724,  1258,     6,  2437,  4363,  1317,   193,    25,     9,\n",
       "          57,    19,    49,   159,    41,     1,  5191,    31,   575,\n",
       "        2079,     5,    90,   171,     7,     1,   499,    49,   856,\n",
       "          37,    16,  9665,  7894,     1,  6041,    88,     9,  4417,\n",
       "          57,    65,    23,    26,    18,    90,   306,    22,     1,\n",
       "          25,    36,     8,    30,  1939,  1235,   124,    29,   834,\n",
       "           1,   834,     1,   171,     1, 17449,     1,  1918,   434,\n",
       "        7577,     1,     1,  5991,     8,  5334,  4891,   580,   624,\n",
       "          57,    19,    30,   150,   426,  4456,   381,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkQdtAZJevf1",
    "outputId": "0a68b1d2-e10a-462a-a067-1b1a3062d4f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 35s 270ms/step - loss: 2.6639 - acc: 0.1424 - val_loss: 2.2283 - val_acc: 0.2161\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 30s 243ms/step - loss: 1.9270 - acc: 0.3389 - val_loss: 1.5123 - val_acc: 0.4931\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 32s 254ms/step - loss: 1.4967 - acc: 0.4799 - val_loss: 1.2483 - val_acc: 0.5591\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 1.2749 - acc: 0.5608 - val_loss: 1.2468 - val_acc: 0.5726\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 30s 243ms/step - loss: 1.1124 - acc: 0.6165 - val_loss: 1.1226 - val_acc: 0.6272\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 32s 253ms/step - loss: 0.9881 - acc: 0.6600 - val_loss: 1.0785 - val_acc: 0.6364\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.8737 - acc: 0.6962 - val_loss: 1.0237 - val_acc: 0.6607\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 31s 251ms/step - loss: 0.7690 - acc: 0.7329 - val_loss: 1.0536 - val_acc: 0.6609\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.6734 - acc: 0.7673 - val_loss: 1.0902 - val_acc: 0.6527\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 33s 266ms/step - loss: 0.5944 - acc: 0.7923 - val_loss: 1.2159 - val_acc: 0.6404\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.5225 - acc: 0.8179 - val_loss: 1.1137 - val_acc: 0.6719\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 33s 266ms/step - loss: 0.4659 - acc: 0.8415 - val_loss: 1.1972 - val_acc: 0.6627\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.3955 - acc: 0.8634 - val_loss: 1.0828 - val_acc: 0.6919\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 33s 266ms/step - loss: 0.3521 - acc: 0.8740 - val_loss: 1.1225 - val_acc: 0.6844\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.3121 - acc: 0.8920 - val_loss: 1.2462 - val_acc: 0.6924\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 33s 266ms/step - loss: 0.2757 - acc: 0.9079 - val_loss: 1.2207 - val_acc: 0.6989\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 30s 241ms/step - loss: 0.2529 - acc: 0.9156 - val_loss: 1.4114 - val_acc: 0.6759\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 31s 248ms/step - loss: 0.2212 - acc: 0.9265 - val_loss: 1.1777 - val_acc: 0.7119\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 38s 305ms/step - loss: 0.2036 - acc: 0.9309 - val_loss: 1.3457 - val_acc: 0.6947\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 31s 249ms/step - loss: 0.2045 - acc: 0.9347 - val_loss: 1.4172 - val_acc: 0.6884\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "hist=model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuW1ZCNBdFed",
    "outputId": "ae2cf562-cd34-4add-a2f6-209548b471f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6638693809509277,\n",
       " 1.9270107746124268,\n",
       " 1.496738076210022,\n",
       " 1.2749024629592896,\n",
       " 1.1124058961868286,\n",
       " 0.988091766834259,\n",
       " 0.8736910820007324,\n",
       " 0.7689957618713379,\n",
       " 0.6734247207641602,\n",
       " 0.5944406986236572,\n",
       " 0.5224837064743042,\n",
       " 0.4658552408218384,\n",
       " 0.3954528868198395,\n",
       " 0.3521294891834259,\n",
       " 0.31205713748931885,\n",
       " 0.27565211057662964,\n",
       " 0.252868115901947,\n",
       " 0.22124083340168,\n",
       " 0.20364879071712494,\n",
       " 0.2045324295759201]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwPC-BFYv5J5"
   },
   "source": [
    "# TASK 1\n",
    "\n",
    "A) **Inference**\n",
    "\n",
    "* So far, we learned how to train a model.  But we do not discuss how to apply it for a new textual instance. We should put the capabilities learned during training to work. We call inference that is applying a machine learning model to a dataset and generating an output or prediction.\n",
    "\n",
    "So write a function **inference(model, textual_input)** to do inference for the CNN based model trained above \"pre-trained word embeddings with CNN\"\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BIWWIvQ447fS"
   },
   "outputs": [],
   "source": [
    "def inference(model, textual_input):\n",
    "    # Vectorize the textual input using the same TextVectorization layer was used during training\n",
    "    vectorized_input = vectorizer(np.array([textual_input])).numpy()\n",
    "    prediction = model.predict(vectorized_input)\n",
    "    predicted_class = class_names[np.argmax(prediction[0])]\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRlzIV_sohQ-",
    "outputId": "5b919853-22d4-4563-fb20-12d52551647a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 152ms/step\n",
      "Predicted class: soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# Inference function Example 1\n",
    "\n",
    "textual_input = \"It doesn\\'t take much looking to see that the U.S. is in a  \\nstate of moral decay.\"\n",
    "predicted_class = inference(model, textual_input)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtXDLYEX-wVp",
    "outputId": "fca634a8-09f0-4ead-ce43-53f37524b955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted class: comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Inference function Example 3\n",
    "\n",
    "textual_input = \"I am interested in learning more about Natural Language Processing.\"\n",
    "predicted_class = inference(model, textual_input)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5kSvUv5ylyn",
    "outputId": "9e7c5297-a4f9-45c6-8c00-109178a8dc33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "Predicted class: rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "# Example usage of inference function\n",
    "\n",
    "# new textual instance-4\n",
    "\n",
    "textual_input = \"This is a test document about sports\"\n",
    "predicted_class = inference(model, textual_input)\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xW3PlA60RpY",
    "outputId": "3f45d965-da50-4192-ba6f-b785fda7750f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "Input text: I am interested in learning more about Natural Language Processing\n",
      "Predicted class: comp.graphics\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Input text: You don't get a new trial because you screwed up and\n",
      "forgot to call all of your witnesses.\n",
      "Predicted class: talk.politics.guns\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Input text: I love playing sports and being active\n",
      "Predicted class: rec.sport.baseball\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another example for Inference function \n",
    "\n",
    "textual_inputs = [\n",
    "    \"I am interested in learning more about Natural Language Processing\",\n",
    "    \"You don't get a new trial because you screwed up and\\nforgot to call all of your witnesses.\",\n",
    "    \"I love playing sports and being active\",\n",
    "]\n",
    "\n",
    "for input_text in textual_inputs:\n",
    "    predicted_class = inference(model, input_text)\n",
    "    print(\"Input text:\", input_text)\n",
    "    print(\"Predicted class:\", predicted_class)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zau7XDnCU6y0"
   },
   "source": [
    "B) **GRADIO** \n",
    "\n",
    "Take a look at gradio and build a demo for your model with a user-friendly web interface so that we can use it. \n",
    "\n",
    "https://gradio.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUh8T1cCAmt4",
    "outputId": "42283049-8480-4819-807f-06638bbcfff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gradio\n",
      "  Downloading gradio-3.23.0-py3-none-any.whl (15.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gradio) (2.27.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson\n",
      "  Downloading orjson-3.8.8-cp39-cp39-manylinux_2_28_x86_64.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.2)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio) (2023.3.0)\n",
      "Collecting websockets>=10.0\n",
      "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.2.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio) (8.4.0)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py[linkify]>=2.0.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting semantic-version\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio) (4.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gradio) (1.22.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from gradio) (6.0)\n",
      "Collecting aiofiles\n",
      "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio) (3.7.1)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio) (1.10.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio) (1.4.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->gradio) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->gradio) (3.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->gradio) (4.65.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1\n",
      "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2022.7.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (22.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.27.0,>=0.26.1\n",
      "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (2022.12.7)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore<0.17.0,>=0.15.0\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sniffio\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (5.12.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (4.39.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (3.4)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio) (8.1.3)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5.0,>=3.0\n",
      "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio) (3.15.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=2d84731b63b58baca19f5947aad9a6b5aa02e1ee54b2ab605f90e0ace1feaea5\n",
      "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: rfc3986, pydub, ffmpy, websockets, uc-micro-py, sniffio, semantic-version, python-multipart, orjson, multidict, mdurl, h11, frozenlist, async-timeout, aiofiles, yarl, uvicorn, markdown-it-py, linkify-it-py, huggingface-hub, anyio, aiosignal, starlette, mdit-py-plugins, httpcore, aiohttp, httpx, fastapi, gradio\n",
      "Successfully installed aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 anyio-3.6.2 async-timeout-4.0.2 fastapi-0.95.0 ffmpy-0.3.0 frozenlist-1.3.3 gradio-3.23.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 huggingface-hub-0.13.3 linkify-it-py-2.0.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 multidict-6.0.4 orjson-3.8.8 pydub-0.25.1 python-multipart-0.0.6 rfc3986-1.5.0 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-10.4 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "id": "vdSTTwnfVSYO",
    "outputId": "7afbc4c8-4d91-4f43-89b7-ad1fb3c3c88e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/usr/local/lib/python3.9/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "(async (port, path, width, height, cache, element) => {\n",
       "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
       "                            return;\n",
       "                        }\n",
       "                        element.appendChild(document.createTextNode(''));\n",
       "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
       "\n",
       "                        const external_link = document.createElement('div');\n",
       "                        external_link.innerHTML = `\n",
       "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
       "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
       "                                    https://localhost:${port}${path}\n",
       "                                </a>\n",
       "                            </div>\n",
       "                        `;\n",
       "                        element.appendChild(external_link);\n",
       "\n",
       "                        const iframe = document.createElement('iframe');\n",
       "                        iframe.src = new URL(path, url).toString();\n",
       "                        iframe.height = height;\n",
       "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
       "                        iframe.width = width;\n",
       "                        iframe.style.border = 0;\n",
       "                        element.appendChild(iframe);\n",
       "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your gradio code goes here\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def predict_category(text):\n",
    "    prediction = inference(model, text)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict_category,\n",
    "    inputs=gr.inputs.Textbox(label=\"Enter text here\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Predicted category\"),\n",
    "    title=\"Text Classification Demo\",\n",
    "    description=\"Enter some text and see which category it belongs to.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "hJ4IL9deVcKq",
    "outputId": "0f4a69c3-9522-4515-d16e-3ff32eaa8c42",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://8addf643791e4b8dea.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8addf643791e4b8dea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a public link\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def predict_category(text):\n",
    "    prediction = inference(model, text)\n",
    "    return prediction\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict_category,\n",
    "    inputs=gr.inputs.Textbox(label=\"Enter text here\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Predicted category\"),\n",
    "    title=\"Text Classification Demo\",\n",
    "    description=\"Enter some text and see which category it belongs to.\",\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "84BfHQ9nE_QU"
   },
   "outputs": [],
   "source": [
    "TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "mWEz5rDXFMaq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8rR7RLFOFNGx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "1DFCmk-UFNkE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4roY4onFN_m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
