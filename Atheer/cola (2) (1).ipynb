{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4W5I1TVikCi",
    "outputId": "03cc5e0a-8759-42f6-8555-1eeaffc6a7df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: torch-geometric in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (4.65.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch-geometric) (1.2.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch-geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch-geometric) (2023.7.22)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Requirement already satisfied: pyarabic in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyarabic) (1.16.0)\n",
      "Requirement already satisfied: arabert in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: PyArabic in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arabert) (0.6.15)\n",
      "Requirement already satisfied: farasapy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arabert) (0.0.14)\n",
      "Requirement already satisfied: emoji==1.4.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arabert) (1.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from farasapy->arabert) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from farasapy->arabert) (4.65.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from PyArabic->arabert) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->farasapy->arabert) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->farasapy->arabert) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->farasapy->arabert) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->farasapy->arabert) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->farasapy->arabert) (0.4.6)\n",
      "Requirement already satisfied: transformers[tf-gpu] in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from transformers[tf-gpu]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[tf-gpu]) (4.65.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[tf-gpu]) (2023.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[tf-gpu]) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers[tf-gpu]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[tf-gpu]) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[tf-gpu]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[tf-gpu]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[tf-gpu]) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: transformers 4.30.2 does not provide the extra 'tf-gpu'\n",
      "fatal: destination path 'arabert' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyArabic in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r arabert/requirements.txt (line 3)) (0.6.15)\n",
      "Requirement already satisfied: farasapy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r arabert/requirements.txt (line 4)) (0.0.14)\n",
      "Requirement already satisfied: emoji==1.4.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r arabert/requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from PyArabic->-r arabert/requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from farasapy->-r arabert/requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from farasapy->-r arabert/requirements.txt (line 4)) (4.65.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->farasapy->-r arabert/requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->farasapy->-r arabert/requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->farasapy->-r arabert/requirements.txt (line 4)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->farasapy->-r arabert/requirements.txt (line 4)) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->farasapy->-r arabert/requirements.txt (line 4)) (0.4.6)\n",
      "Collecting farasa\n",
      "  Downloading Farasa-0.0.1-py2.py3-none-any.whl (12.6 MB)\n",
      "     ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.6 MB 145.2 kB/s eta 0:01:27\n",
      "     --------------------------------------- 0.1/12.6 MB 252.2 kB/s eta 0:00:50\n",
      "     --------------------------------------- 0.1/12.6 MB 327.7 kB/s eta 0:00:39\n",
      "     --------------------------------------- 0.2/12.6 MB 482.7 kB/s eta 0:00:26\n",
      "      -------------------------------------- 0.2/12.6 MB 570.5 kB/s eta 0:00:22\n",
      "      -------------------------------------- 0.3/12.6 MB 707.1 kB/s eta 0:00:18\n",
      "     - ------------------------------------- 0.4/12.6 MB 791.2 kB/s eta 0:00:16\n",
      "     - ------------------------------------- 0.4/12.6 MB 839.7 kB/s eta 0:00:15\n",
      "     - ------------------------------------- 0.5/12.6 MB 954.7 kB/s eta 0:00:13\n",
      "     -- ------------------------------------- 0.7/12.6 MB 1.1 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.7/12.6 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.9/12.6 MB 1.3 MB/s eta 0:00:10\n",
      "     --- ------------------------------------ 1.0/12.6 MB 1.3 MB/s eta 0:00:10\n",
      "     --- ------------------------------------ 1.0/12.6 MB 1.3 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.1/12.6 MB 1.3 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.2/12.6 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.3/12.6 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.6 MB 1.4 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.5/12.6 MB 1.5 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.6/12.6 MB 1.5 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.6/12.6 MB 1.5 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.7/12.6 MB 1.5 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.8/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.9/12.6 MB 1.6 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.0/12.6 MB 1.3 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 2.8/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.9/12.6 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.9/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.9/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.9/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.0/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.3/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.4/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.4/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.5/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.6/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.7/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.8/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.9/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.0/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.1/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.2/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.3/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.4/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.5/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.7/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 4.8/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 4.9/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 4.9/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.0/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.1/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.2/12.6 MB 1.8 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.3/12.6 MB 1.8 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.4/12.6 MB 1.8 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.5/12.6 MB 1.8 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.6/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.7/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.0/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.1/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.2/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.3/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.4/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.5/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.7/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.8/12.6 MB 1.9 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.9/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.0/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.2/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.3/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.4/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.5/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.6/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.7/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 7.9/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.6 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.1/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.2/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.3/12.6 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.4/12.6 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.5/12.6 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.6/12.6 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.7/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 8.8/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 8.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.0/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.1/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.2/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.3/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.5/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.6/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.8/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.6 MB 1.9 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.7/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.8/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.8/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.0/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.1/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.4/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.7/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.8/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.0/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.1/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.6 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.3/12.6 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.4/12.6 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.4/12.6 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.5/12.6 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.6 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.6/12.6 MB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: farasa\n",
      "Successfully installed farasa-0.0.1\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: dgl in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dgl) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dgl) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dgl) (3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dgl) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dgl) (4.65.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from dgl) (5.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->dgl) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->dgl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->dgl) (0.4.6)\n",
      "Requirement already satisfied: stanza in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (4.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (2.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (4.65.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Collecting en-core-web-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 108.9 kB/s eta 0:01:58\n",
      "     --------------------------------------- 0.0/12.8 MB 108.9 kB/s eta 0:01:58\n",
      "     --------------------------------------- 0.0/12.8 MB 108.9 kB/s eta 0:01:58\n",
      "     --------------------------------------- 0.0/12.8 MB 100.9 kB/s eta 0:02:07\n",
      "     --------------------------------------- 0.0/12.8 MB 115.5 kB/s eta 0:01:51\n",
      "     --------------------------------------- 0.1/12.8 MB 163.6 kB/s eta 0:01:18\n",
      "     --------------------------------------- 0.1/12.8 MB 284.4 kB/s eta 0:00:45\n",
      "      -------------------------------------- 0.2/12.8 MB 388.2 kB/s eta 0:00:33\n",
      "      -------------------------------------- 0.3/12.8 MB 524.0 kB/s eta 0:00:24\n",
      "      -------------------------------------- 0.3/12.8 MB 612.3 kB/s eta 0:00:21\n",
      "     - ------------------------------------- 0.4/12.8 MB 730.1 kB/s eta 0:00:17\n",
      "     - ------------------------------------- 0.5/12.8 MB 827.5 kB/s eta 0:00:15\n",
      "     - ------------------------------------- 0.6/12.8 MB 894.6 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.7/12.8 MB 982.3 kB/s eta 0:00:13\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 0.9/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     --- ------------------------------------ 1.2/12.8 MB 1.3 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.3/12.8 MB 1.3 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.3 MB/s eta 0:00:09\n",
      "     ------ --------------------------------- 2.2/12.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.5 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.7/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 4.0/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.0/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.1/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.3/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.4/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.5/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.6/12.8 MB 1.6 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.6 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 4.9/12.8 MB 1.7 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.7 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.9/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 6.1/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.1/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.2/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.3/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.5/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.5/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.7/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.7/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 6.9/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 7.0/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.4/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 8.2/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.2/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.5/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.5/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.6/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.8/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.8/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.6/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.8/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.9/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.0/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.1/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.4/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.6/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.8/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.5/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.65.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.23.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.3)\n",
      "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#installations\n",
    "!pip install transformers\n",
    "!pip install torch-geometric\n",
    "!pip install pyarabic\n",
    "!pip install arabert\n",
    "!pip install transformers[tf-gpu]\n",
    "!git clone https://github.com/aub-mind/arabert\n",
    "!pip install -r arabert/requirements.txt\n",
    "!pip install farasa\n",
    "!pip install torch\n",
    "!pip install dgl\n",
    "!pip install stanza\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hcTcdSUQjdp4"
   },
   "outputs": [],
   "source": [
    "# nlp = stanza.Pipeline('ar')  # Initialize the Arabic pipeline for dependency parsing\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Linear, LSTM\n",
    "from dgl.nn.pytorch import GATConv\n",
    "import spacy\n",
    "import warnings\n",
    "import stanza\n",
    "import torch\n",
    "import spacy\n",
    "import numpy as np\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model_name = 'aubmindlab/bert-base-arabertv01'\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.955, Validation Accuracy = 71.88%\n",
      "Epoch 2: Train Loss = 0.734, Validation Accuracy = 65.62%\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def get_dependency_tree(tokens):\n",
    "  doc = nlp(' '.join(tokens))\n",
    "  return [(token.dep_, token.head.i, token.i) for token in doc]\n",
    "\n",
    "# Read training data\n",
    "train_dataset = pd.read_csv('My_AR_HOTE_SB1_TRAIN (1).csv')\n",
    "train_dataset.dropna(subset=['Polarity'], inplace=True)\n",
    "train_dataset['Polarity'] = train_dataset['Polarity'].map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "train_dataset = train_dataset[:200]\n",
    "\n",
    "test_dataset = pd.read_csv('My_AR_HOTE_SB1_TEST (1).csv')\n",
    "test_dataset.dropna(subset=['Polarity'], inplace=True)\n",
    "test_dataset['Polarity'] = test_dataset['Polarity'].map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "# test_dataset = test_dataset[:16]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "class CustomDataset():\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the text, term, and polarity\n",
    "        text = self.X.iloc[index]['Text']\n",
    "        term = self.X.iloc[index]['Term']\n",
    "        polarity = self.y.iloc[index]\n",
    "\n",
    "        # Preprocess text and term\n",
    "        text = arabert_prep.preprocess(text)\n",
    "        term = arabert_prep.preprocess(term)\n",
    "\n",
    "        return text , term , polarity\n",
    "\n",
    "class TwoDirectionalModel(Module):\n",
    "    def __init__(self,batch_size, text_embedding_dim, term_embedding_dim, lstm_hidden_dim, gat_hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gat_hidden_dim = gat_hidden_dim\n",
    "\n",
    "        # Term input layers\n",
    "        self.term_lstm = nn.LSTM(term_embedding_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.term_fc = Linear(2*lstm_hidden_dim, gat_hidden_dim)\n",
    "\n",
    "        # Context input layers\n",
    "        self.text_lstm = nn.LSTM(text_embedding_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.text_fc = Linear(2*lstm_hidden_dim, gat_hidden_dim)\n",
    "\n",
    "        # GAT layer for text data\n",
    "        self.text_gat = GATConv(in_feats=gat_hidden_dim,\n",
    "                                out_feats=512,\n",
    "                                num_heads=batch_size)\n",
    "\n",
    "        # GAT layer for term data\n",
    "        self.term_gat = GATConv(in_feats=gat_hidden_dim,\n",
    "                                out_feats=512,\n",
    "                                num_heads=batch_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, train_term_outputs, train_text_outputs, text_token, term_token):\n",
    "        text_dep = []\n",
    "        for i in text_token:\n",
    "            text_g = dgl.DGLGraph()\n",
    "            text_g.add_nodes(len(i))\n",
    "            text_dependency_graph = get_dependency_tree(i)\n",
    "            parent, child = zip(*[(p, c) for r, p, c in text_dependency_graph])\n",
    "            text_g.add_edges(parent, child)\n",
    "            text_dep.append(text_g)\n",
    "\n",
    "        term_dep = []\n",
    "        for i in term_token:\n",
    "            term_g = dgl.DGLGraph()\n",
    "            term_g.add_nodes(len(i))\n",
    "            term_dependency_graph = get_dependency_tree(i)\n",
    "            parent, child = zip(*[(p, c) for r, p, c in term_dependency_graph])\n",
    "            term_g.add_edges(parent, child)\n",
    "            term_dep.append(term_g)\n",
    "\n",
    "        \n",
    "        # Term input LSTM\n",
    "        train_term_outputs = torch.cat(train_term_outputs, dim=0)\n",
    "        term_outputs, _ = self.term_lstm(train_term_outputs)\n",
    "        term_outputs = self.term_fc(term_outputs)\n",
    "\n",
    "        # Text input LSTM\n",
    "        train_text_outputs = torch.cat(train_text_outputs, dim=0)\n",
    "        text_outputs, _ = self.text_lstm(train_text_outputs)\n",
    "        text_outputs = self.text_fc(text_outputs)\n",
    "\n",
    "        # Text GAT layer\n",
    "        text_dep = dgl.batch(text_dep)\n",
    "\n",
    "        text_outputs = self.text_gat(text_dep, text_outputs)\n",
    "\n",
    "        # Term GAT layer\n",
    "        term_dep = dgl.batch(term_dep)\n",
    "        term_outputs = self.term_gat(term_dep, term_outputs)\n",
    "\n",
    "        # Graph mean node representation\n",
    "        context_outputs = torch.mean(text_outputs, dim=0 )\n",
    "        term_outputs = torch.mean(term_outputs, dim=0)\n",
    "        # Concatenate text and term GAT outputs and use as input to output layer\n",
    "        outputs = torch.cat([term_outputs, context_outputs], dim=1)\n",
    "        outputs = self.fc(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(train_dataset.loc[:,['Text','Term']], train_dataset['Polarity'], test_size=0.2, random_state=42)\n",
    "X_test, y_text = test_dataset.loc[:,['Text','Term']],test_dataset['Polarity']\n",
    "# Create dataloaders for training and testing data\n",
    "train_data = CustomDataset(X_train,y_train)\n",
    "val_data = CustomDataset(X_val,y_val)\n",
    "test_data = CustomDataset(X_test, y_text)\n",
    "train_dataloader = DataLoader(train_data, batch_size=16)\n",
    "val_dataloader = DataLoader(val_data, batch_size=16)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = TwoDirectionalModel(batch_size=16,term_embedding_dim=300, text_embedding_dim=300, lstm_hidden_dim=128, gat_hidden_dim=256, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for text_outputs,term_outputs , polarity in train_dataloader:\n",
    "        text_token_list= []\n",
    "        term_token_list =[]\n",
    "        term_outputs_emp=[]\n",
    "        text_outputs_emp=[]\n",
    "        # Define a dictionary to map out-of-vocabulary tokens to a special <unk> token\n",
    "        unk_token = \"<unk>\"\n",
    "        text_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "        text_num_embeddings = 1\n",
    "        for text in text_outputs:\n",
    "            text_token = [token.text for token in nlp(text)][:5]\n",
    "            # Add new word to `text_vocab` if not already there\n",
    "            for token in text_token:\n",
    "                if token not in text_vocab:\n",
    "                    text_vocab[token] = text_num_embeddings\n",
    "                    text_num_embeddings += 1\n",
    "            text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text_token]\n",
    "            text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "            text_token_list.append(text_token)\n",
    "            text_outputs_emp.append(text_input_emb)\n",
    "        term_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "        term_num_embeddings = 1\n",
    "        for term in term_outputs:\n",
    "            term_token = [token.text for token in nlp(term)][:1]\n",
    "            # Add new word to `text_vocab` if not already there\n",
    "            for token in term_token:\n",
    "                if token not in term_vocab:\n",
    "                    term_vocab[token] = term_num_embeddings\n",
    "                    term_num_embeddings += 1\n",
    "            term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term_token]\n",
    "            term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "            term_token_list.append(term_token)\n",
    "            term_outputs_emp.append(term_input_emb)\n",
    "\n",
    "        # Call the model forward method with concatenated tensors\n",
    "        logits = model( term_outputs_emp, text_outputs_emp, text_token_list, term_token_list)\n",
    "        # Compute logits and loss\n",
    "        train_polarity=polarity\n",
    "        if(logits.shape[0] == train_polarity.shape[0]) :\n",
    "            loss = criterion(logits, train_polarity.long())\n",
    "\n",
    "            # Backpropagation and weight updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for text_outputs , term_outputs , polarity in val_dataloader:\n",
    "            test_polarity=polarity\n",
    "            # Compute logits and accuracy\n",
    "            text_token_list= []\n",
    "            term_token_list =[]\n",
    "            term_outputs_emp=[]\n",
    "            text_outputs_emp=[]\n",
    "            # Define a dictionary to map out-of-vocabulary tokens to a special <unk> token\n",
    "            unk_token = \"<unk>\"\n",
    "            text_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            text_num_embeddings = 1\n",
    "            for text in text_outputs:\n",
    "                text_token = [token.text for token in nlp(text)][:5]\n",
    "                # Add new word to `text_vocab` if not already there\n",
    "                for token in text_token:\n",
    "                    if token not in text_vocab:\n",
    "                        text_vocab[token] = text_num_embeddings\n",
    "                        text_num_embeddings += 1\n",
    "                text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "                # Map out-of-vocabulary tokens to <unk> token\n",
    "                text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text_token]\n",
    "                text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "                text_token_list.append(text_token)\n",
    "                text_outputs_emp.append(text_input_emb)\n",
    "            term_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            term_num_embeddings = 1\n",
    "            for term in term_outputs:\n",
    "                term_token = [token.text for token in nlp(term)][:1]\n",
    "                # Add new word to `text_vocab` if not already there\n",
    "                for token in term_token:\n",
    "                    if token not in term_vocab:\n",
    "                        term_vocab[token] = term_num_embeddings\n",
    "                        term_num_embeddings += 1\n",
    "                term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "                # Map out-of-vocabulary tokens to <unk> token\n",
    "                term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term_token]\n",
    "                term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "                term_token_list.append(term_token)\n",
    "                term_outputs_emp.append(term_input_emb)\n",
    "            logits = model(term_outputs_emp, text_outputs_emp, text_token_list, term_token_list)\n",
    "            if (test_polarity.shape[0] == logits.shape[0]):\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += test_polarity.size(0)\n",
    "                correct += (predicted == test_polarity).sum().item()\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {running_loss/len(train_dataloader):.3f}, Validation Accuracy = {(100*correct/total):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 54.55%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.57      0.88      0.69      1500\n",
      "    negative       0.33      0.11      0.16       923\n",
      "     neutral       0.00      0.00      0.00       169\n",
      "\n",
      "    accuracy                           0.55      2592\n",
      "   macro avg       0.30      0.33      0.29      2592\n",
      "weighted avg       0.45      0.55      0.46      2592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize variables for accuracy computation\n",
    "y_label = []\n",
    "y_predict = []\n",
    "correct = 0\n",
    "total = 0\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Loop through batches in dataloader\n",
    "    for text_outputs, term_outputs, polarity in test_dataloader:\n",
    "        \n",
    "        # Preprocess inputs (tokenize and embed)\n",
    "        text_token_list = []\n",
    "        term_token_list = []\n",
    "        term_outputs_emp = []\n",
    "        text_outputs_emp = []\n",
    "        unk_token = \"<unk>\"\n",
    "        for text in text_outputs:\n",
    "            text_token = [token.text for token in nlp(text)][:5]\n",
    "            for token in text_token:\n",
    "                if token not in text_vocab:\n",
    "                    text_vocab[token] = text_num_embeddings\n",
    "                    text_num_embeddings += 1\n",
    "            text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "            text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text_token]\n",
    "            text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "            text_token_list.append(text_token)\n",
    "            text_outputs_emp.append(text_input_emb)\n",
    "        term_vocab = {unk_token: 0}\n",
    "        term_num_embeddings = 1\n",
    "        for term in term_outputs:\n",
    "            term_token = [token.text for token in nlp(term)][:1]\n",
    "            for token in term_token:\n",
    "                if token not in term_vocab:\n",
    "                    term_vocab[token] = term_num_embeddings\n",
    "                    term_num_embeddings += 1\n",
    "            term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "            term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term_token]\n",
    "            term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "            term_token_list.append(term_token)\n",
    "            term_outputs_emp.append(term_input_emb)\n",
    "\n",
    "        # Call forward method and get logits\n",
    "        logits = model(term_outputs_emp, text_outputs_emp, text_token_list, term_token_list)\n",
    "        test_polarity = polarity\n",
    "        if (test_polarity.shape[0] == logits.shape[0]):\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += test_polarity.size(0)\n",
    "            correct += (predicted == test_polarity).sum().item()\n",
    "            y_label.extend(test_polarity)\n",
    "            y_predict.extend(predicted)\n",
    "# Print accuracy\n",
    "print(f\"Test Accuracy = {(100*correct/total):.2f}%\")\n",
    "plot_labels = ['positive', 'negative', 'neutral']\n",
    "report = classification_report(y_label, y_predict, target_names=plot_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "stanford_parser_dir = \"C:/Users/hP/Desktop/lo/stanford-corenlp-4.2.0-models-arabic.jar\"\n",
    "stanford_model_dir = \"C:/Users/hP/Desktop/lo/stanford-parser-full-2020-11-17/stanford-parser.jar\"\n",
    "java_options = '-mx1g'\n",
    "stanford_parser = StanfordDependencyParser(\n",
    "    stanford_parser_dir,\n",
    "    stanford_model_dir,\n",
    "    java_options=java_options\n",
    ")\n",
    "\n",
    "def procc(text):\n",
    "    parsed_sents = stanford_parser.raw_parse(text)\n",
    "    dep_tree = next(parsed_sents).to_conll(4)\n",
    "    g = dgl.DGLGraph()\n",
    "    words = []\n",
    "    deps = []\n",
    "    labels = {}\n",
    "    for line in dep_tree.split('\\n'):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) != 4:\n",
    "            continue\n",
    "        word = parts[0]\n",
    "        pos = parts[1]\n",
    "        relation = parts[2]\n",
    "        if relation not in labels:\n",
    "            labels[relation] = len(labels)\n",
    "        try:\n",
    "            parent_idx = int(parts[3]) - 1\n",
    "        except ValueError:\n",
    "            parent_idx = -1\n",
    "        words.append(word)\n",
    "        deps.append((parent_idx, len(words) - 1, labels[relation]))  # Use the label instead of the string relation\n",
    "    g.add_nodes(len(words))\n",
    "    src, dst, rel = zip(*deps)\n",
    "    g.add_edges(src, dst)\n",
    "    g.edata['rel'] = torch.tensor(rel)\n",
    "\n",
    "    return words ,g\n",
    "\n",
    "train_dataset = pd.read_csv('My_AR_HOTE_SB1_TRAIN (1).csv')\n",
    "train_dataset.dropna(subset=['Polarity'], inplace=True)\n",
    "train_dataset['Polarity'] = train_dataset['Polarity'].map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "train_dataset = train_dataset[:200]\n",
    "\n",
    "test_dataset = pd.read_csv('My_AR_HOTE_SB1_TEST (1).csv')\n",
    "test_dataset.dropna(subset=['Polarity'], inplace=True)\n",
    "test_dataset['Polarity'] = test_dataset['Polarity'].map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "test_dataset = test_dataset[:16]\n",
    "\n",
    "\n",
    "class CustomDataset():\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the text, term, and polarity\n",
    "        text = self.X.iloc[index]['Text']\n",
    "        term = self.X.iloc[index]['Term']\n",
    "        polarity = self.y.iloc[index]\n",
    "\n",
    "        # Preprocess text and term\n",
    "        text = arabert_prep.preprocess(text)\n",
    "        term = arabert_prep.preprocess(term)\n",
    "\n",
    "        return text , term , polarity\n",
    "\n",
    "class TwoDirectionalModel(Module):\n",
    "    def __init__(self,batch_size, text_embedding_dim, term_embedding_dim, lstm_hidden_dim, gat_hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gat_hidden_dim = gat_hidden_dim\n",
    "\n",
    "        # Term input layers\n",
    "        self.term_lstm = nn.LSTM(term_embedding_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.term_fc = Linear(2*lstm_hidden_dim, gat_hidden_dim)\n",
    "\n",
    "        # Context input layers\n",
    "        self.text_lstm = nn.LSTM(text_embedding_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.text_fc = Linear(2*lstm_hidden_dim, gat_hidden_dim)\n",
    "\n",
    "        # GAT layer for text data\n",
    "        self.text_gat = GATConv(in_feats=gat_hidden_dim,\n",
    "                                out_feats=512,\n",
    "                                num_heads=batch_size)\n",
    "\n",
    "        # GAT layer for term data\n",
    "        self.term_gat = GATConv(in_feats=gat_hidden_dim,\n",
    "                                out_feats=512,\n",
    "                                num_heads=batch_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, train_term_outputs, train_text_outputs, text_dep, term_dep):        \n",
    "        # Term input LSTM\n",
    "        train_term_outputs = torch.cat(train_term_outputs, dim=0)\n",
    "        term_outputs, _ = self.term_lstm(train_term_outputs)\n",
    "        term_outputs = self.term_fc(term_outputs)\n",
    "        # Text input LSTM\n",
    "        train_text_outputs = torch.cat(train_text_outputs, dim=0)\n",
    "        text_outputs, _ = self.text_lstm(train_text_outputs)\n",
    "        text_outputs = self.text_fc(text_outputs)\n",
    "        print(\"g\")\n",
    "        # Text GAT layer\n",
    "        text_dep = dgl.batch(text_dep)\n",
    "        print(text_dep)\n",
    "        print(text_outputs)\n",
    "        text_outputs = self.text_gat(text_dep, text_outputs)\n",
    "        # Term GAT layer\n",
    "        term_dep = dgl.batch(term_dep)\n",
    "        term_outputs = self.term_gat(term_dep, term_outputs)\n",
    "\n",
    "        # Graph mean node representation\n",
    "        context_outputs = torch.mean(text_outputs, dim=0 )\n",
    "        term_outputs = torch.mean(term_outputs, dim=0)\n",
    "        # Concatenate text and term GAT outputs and use as input to output layer\n",
    "        outputs = torch.cat([term_outputs, context_outputs], dim=1)\n",
    "        outputs = self.fc(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Create dataloaders for training and testing data\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_dataset.loc[:,['Text','Term']], train_dataset['Polarity'], test_size=0.2, random_state=42)\n",
    "X_test, y_text = test_dataset.loc[:,['Text','Term']],test_dataset['Polarity']\n",
    "# Create dataloaders for training and testing data\n",
    "train_data = CustomDataset(X_train,y_train)\n",
    "val_data = CustomDataset(X_val,y_val)\n",
    "test_data = CustomDataset(X_test, y_text)\n",
    "train_dataloader = DataLoader(train_data, batch_size=16)\n",
    "val_dataloader = DataLoader(val_data, batch_size=16)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16)\n",
    "# Define the model, loss function, and optimizer\n",
    "tokenizer.model_max_length = 300\n",
    "model = TwoDirectionalModel(batch_size=16,term_embedding_dim=tokenizer.model_max_length, text_embedding_dim=tokenizer.model_max_length, lstm_hidden_dim=128, gat_hidden_dim=256, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for text_outputs, term_outputs , polarity in train_dataloader:\n",
    "        # Check that the size of text_outputs and term_outputs are equal\n",
    "        if len(text_outputs) != len(term_outputs):\n",
    "            continue\n",
    "        text_dep_list = []\n",
    "        term_dep_list = []\n",
    "        term_outputs_emp = []\n",
    "        text_outputs_emp = []\n",
    "        for i in range(len(text_outputs)):\n",
    "            text = text_outputs[i]\n",
    "            term = term_outputs[i]\n",
    "            unk_token = \"<unk>\"\n",
    "            text_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            text_num_embeddings = 1\n",
    "            text,text_dep= procc(text)\n",
    "            term,term_dep=procc(term)\n",
    "            for token in text:\n",
    "                if token not in text_vocab:\n",
    "                    text_vocab[token] = text_num_embeddings\n",
    "                    text_num_embeddings += 1\n",
    "            text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text]\n",
    "            text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "            text_dep_list.append(text_dep)\n",
    "            text_outputs_emp.append(text_input_emb)\n",
    "            unk_token = \"<unk>\"\n",
    "            term_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            term_num_embeddings = 1\n",
    "            for token in term:\n",
    "                if token not in term_vocab:\n",
    "                    term_vocab[token] = term_num_embeddings\n",
    "                    term_num_embeddings += 1\n",
    "            term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term]\n",
    "            term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "            term_dep_list.append(term_dep)\n",
    "            term_outputs_emp.append(term_input_emb)\n",
    "        # Call the model forward method with concatenated tensors\n",
    "        logits = model(term_outputs_emp, text_outputs_emp, text_dep_list, term_dep_list)\n",
    "        # Compute logits and loss\n",
    "        train_polarity = polarity\n",
    "        if(logits.shape[0] == train_polarity.shape[0]) :\n",
    "          loss = criterion(logits, train_polarity.long())\n",
    "          # Backpropagation and weight updates\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          running_loss += loss.item()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for text_outputs , term_outputs , polarity in val_dataloader:        \n",
    "          text_dep_list = []\n",
    "          term_dep_list = []\n",
    "          term_outputs_emp = []\n",
    "          text_outputs_emp = []\n",
    "          for i in range(len(text_outputs)):\n",
    "              text = text_outputs[i]\n",
    "              term = term_outputs[i]\n",
    "              unk_token = \"<unk>\"\n",
    "              text_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "              text_num_embeddings = 1\n",
    "              text,text_dep= procc(text)\n",
    "              term,term_dep=procc(term)\n",
    "              for token in text:\n",
    "                  if token not in text_vocab:\n",
    "                      text_vocab[token] = text_num_embeddings\n",
    "                      text_num_embeddings += 1\n",
    "              text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "              text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text]\n",
    "              text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "              text_dep_list.append(text_dep)\n",
    "              text_outputs_emp.append(text_input_emb)\n",
    "              unk_token = \"<unk>\"\n",
    "              term_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "              term_num_embeddings = 1\n",
    "              for token in term:\n",
    "                  if token not in term_vocab:\n",
    "                      term_vocab[token] = term_num_embeddings\n",
    "                      term_num_embeddings += 1\n",
    "              term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "              term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term]\n",
    "              term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "              term_dep_list.append(term_dep)\n",
    "              term_outputs_emp.append(term_input_emb)\n",
    "          logits = model(term_outputs_emp, text_outputs_emp, text_dep_list, term_dep_list)\n",
    "          test_polarity = polarity \n",
    "          if (test_polarity.shape[0] == logits.shape[0]):\n",
    "              _, predicted = torch.max(logits.data, 1)\n",
    "              total += test_polarity.size(0)\n",
    "              correct += (predicted == test_polarity).sum().item()\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {running_loss/len(train_dataloader):.3f}, Validation Accuracy = {(100*correct/total):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize variables for accuracy computation\n",
    "y_label = []\n",
    "y_predict = []\n",
    "correct = 0\n",
    "total = 0\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Loop through batches in dataloader\n",
    "    for text_outputs, term_outputs, polarity in test_dataloader:\n",
    "        text_dep_list = []\n",
    "        term_dep_list = []\n",
    "        term_outputs_emp = []\n",
    "        text_outputs_emp = []\n",
    "        for i in range(len(text_outputs)):\n",
    "            text = text_outputs[i]\n",
    "            term = term_outputs[i]\n",
    "            unk_token = \"<unk>\"\n",
    "            text_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            text_num_embeddings = 1\n",
    "            text,text_dep= procc(text)\n",
    "            term,term_dep=procc(term)\n",
    "            for token in text:\n",
    "                if token not in text_vocab:\n",
    "                    text_vocab[token] = text_num_embeddings\n",
    "                    text_num_embeddings += 1\n",
    "            text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "        # Map out-of-vocabulary tokens to <unk> token\n",
    "            text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text]\n",
    "            text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "            text_dep_list.append(text_dep)\n",
    "            text_outputs_emp.append(text_input_emb)\n",
    "            unk_token = \"<unk>\"\n",
    "            term_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            term_num_embeddings = 1\n",
    "            for token in term:\n",
    "                if token not in term_vocab:\n",
    "                    term_vocab[token] = term_num_embeddings\n",
    "                    term_num_embeddings += 1\n",
    "            term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "        # Map out-of-vocabulary tokens to <unk> token\n",
    "            term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term]\n",
    "            term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "            term_dep_list.append(term_dep)\n",
    "            term_outputs_emp.append(term_input_emb)\n",
    "        logits = model(term_outputs_emp, text_outputs_emp, text_dep_list, term_dep_list)\n",
    "        test_polarity = polarity \n",
    "        if (test_polarity.shape[0] == logits.shape[0]):\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += test_polarity.size(0)\n",
    "            correct += (predicted == test_polarity).sum().item()\n",
    "            y_label.extend(test_polarity)\n",
    "            y_predict.extend(predicted)\n",
    "# Print accuracy\n",
    "print(f\"Test Accuracy = {(100*correct/total):.2f}%\")\n",
    "plot_labels = ['positive', 'negative', 'neutral']\n",
    "report = classification_report(y_label, y_predict, target_names=plot_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2023-11-05 17:12:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 1.29MB/s]                    \n",
      "2023-11-05 17:12:21 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "=============================\n",
      "\n",
      "2023-11-05 17:12:21 INFO: Using device: cpu\n",
      "2023-11-05 17:12:21 INFO: Loading: tokenize\n",
      "2023-11-05 17:12:21 INFO: Loading: pos\n",
      "2023-11-05 17:12:22 INFO: Loading: lemma\n",
      "2023-11-05 17:12:22 INFO: Loading: depparse\n",
      "2023-11-05 17:12:23 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dgl\\heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Define a function to get the dependency tree of a sentence using StanfordNLP\n",
    "nlp = stanza.Pipeline(lang='ar', processors='tokenize, lemma, pos, depparse', tokenize_pretokenized=True , tokenize_batch_size=128, tokenize_max_length=5)\n",
    "def get_dependency_tree(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    doc = nlp(text)\n",
    "    tree = [(word.deprel, word.head - 1, word.id - 1) for sentence in doc.sentences for word in sentence.words]\n",
    "    return tree\n",
    "\n",
    "\n",
    "def tokenize_text_arabic(text, max_len=5):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Read training and testing data\n",
    "train_dataset = pd.read_csv('My_AR_HOTE_SB1_TRAIN (1).csv')\n",
    "train_dataset.dropna(subset=['Polarity'], inplace=True)\n",
    "train_dataset['Polarity'] = train_dataset['Polarity'].map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "train_dataset = train_dataset[:32]\n",
    "\n",
    "test_dataset = pd.read_csv('My_AR_HOTE_SB1_TEST (1).csv')\n",
    "test_dataset.dropna(subset=['Polarity'], inplace=True)\n",
    "test_dataset['Polarity'] = test_dataset['Polarity'].map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "test_dataset = test_dataset[:16]\n",
    "\n",
    "# Initialize Farasa POS tagger\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "\n",
    "class CustomDataset():\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the text, term, and polarity\n",
    "        text = self.X.iloc[index]['Text']\n",
    "        term = self.X.iloc[index]['Term']\n",
    "        polarity = self.y.iloc[index]\n",
    "\n",
    "        # Preprocess text and term\n",
    "        text = arabert_prep.preprocess(text)\n",
    "        term = arabert_prep.preprocess(term)\n",
    "\n",
    "        return text , term , polarity\n",
    "\n",
    "\n",
    "class TwoDirectionalModel(Module):\n",
    "    def __init__(self,batch_size, text_embedding_dim, term_embedding_dim, lstm_hidden_dim, gat_hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gat_hidden_dim = gat_hidden_dim\n",
    "\n",
    "        # Term input layers\n",
    "        self.term_lstm = nn.LSTM(term_embedding_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.term_fc = Linear(2*lstm_hidden_dim, gat_hidden_dim)\n",
    "\n",
    "        # Context input layers\n",
    "        self.text_lstm = nn.LSTM(text_embedding_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.text_fc = Linear(2*lstm_hidden_dim, gat_hidden_dim)\n",
    "\n",
    "        # GAT layer for text data\n",
    "        self.text_gat = GATConv(in_feats=gat_hidden_dim,\n",
    "                                out_feats=512,\n",
    "                                num_heads=batch_size)\n",
    "\n",
    "        # GAT layer for term data\n",
    "        self.term_gat = GATConv(in_feats=gat_hidden_dim,\n",
    "                                out_feats=512,\n",
    "                                num_heads=batch_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, train_term_outputs, train_text_outputs, text_token, term_token):\n",
    "        text_dep = []\n",
    "        for i in text_token:\n",
    "            text_g = dgl.DGLGraph()\n",
    "            text_g.add_nodes(len(i))\n",
    "            text_dependency_graph = get_dependency_tree(i)\n",
    "            parent, child = zip(*[(p, c) for r, p, c in text_dependency_graph])\n",
    "            text_g.add_edges(parent, child)\n",
    "            text_dep.append(text_g)\n",
    "\n",
    "        term_dep = []\n",
    "        for i in term_token:\n",
    "            term_g = dgl.DGLGraph()\n",
    "            term_g.add_nodes(len(i))\n",
    "            term_dependency_graph = get_dependency_tree(i)\n",
    "            parent, child = zip(*[(p, c) for r, p, c in term_dependency_graph])\n",
    "            term_g.add_edges(parent, child)\n",
    "            term_dep.append(term_g)\n",
    "        \n",
    "        # Term input LSTM\n",
    "        train_term_outputs = torch.cat(train_term_outputs, dim=0)\n",
    "        term_outputs, _ = self.term_lstm(train_term_outputs)\n",
    "        term_outputs = self.term_fc(term_outputs)\n",
    "        # Text input LSTM\n",
    "        train_text_outputs = torch.cat(train_text_outputs, dim=0)\n",
    "        text_outputs, _ = self.text_lstm(train_text_outputs)\n",
    "        text_outputs = self.text_fc(text_outputs)\n",
    "        # Text GAT layer\n",
    "        text_dep = dgl.batch(text_dep)\n",
    "        text_outputs = self.text_gat(text_dep, text_outputs)\n",
    "        # Term GAT layer\n",
    "        term_dep = dgl.batch(term_dep)\n",
    "        term_outputs = self.term_gat(term_dep, term_outputs)\n",
    "\n",
    "        # Graph mean node representation\n",
    "        context_outputs = torch.mean(text_outputs, dim=0 )\n",
    "        term_outputs = torch.mean(term_outputs, dim=0)\n",
    "        # Concatenate text and term GAT outputs and use as input to output layer\n",
    "        outputs = torch.cat([term_outputs, context_outputs], dim=1)\n",
    "        outputs = self.fc(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_dataset.loc[:,['Text','Term']], train_dataset['Polarity'], test_size=0.2, random_state=42)\n",
    "X_test, y_text = test_dataset.loc[:,['Text','Term']],test_dataset['Polarity']\n",
    "# Create dataloaders for training and testing data\n",
    "train_data = CustomDataset(X_train,y_train)\n",
    "val_data = CustomDataset(X_val,y_val)\n",
    "test_data = CustomDataset(X_test, y_text)\n",
    "train_dataloader = DataLoader(train_data, batch_size=16)\n",
    "val_dataloader = DataLoader(val_data, batch_size=16)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16)\n",
    "# Define the model, loss function, and optimizer\n",
    "tokenizer.model_max_length = 300\n",
    "print(tokenizer.model_max_length)\n",
    "model = TwoDirectionalModel(batch_size=16,term_embedding_dim=tokenizer.model_max_length, text_embedding_dim=tokenizer.model_max_length, lstm_hidden_dim=128, gat_hidden_dim=256, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for text_outputs, term_outputs, Polarity in train_dataloader:\n",
    "        # Check that the size of text_outputs and term_outputs are equal\n",
    "        if len(text_outputs) != len(term_outputs):\n",
    "            continue\n",
    "        text_token_list = []\n",
    "        term_token_list = []\n",
    "        term_outputs_emp = []\n",
    "        text_outputs_emp = []\n",
    "        for i in range(len(text_outputs)):\n",
    "            text = text_outputs[i]\n",
    "            term = term_outputs[i]\n",
    "            unk_token = \"<unk>\"\n",
    "            text_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            text_num_embeddings = 1\n",
    "            text= tokenize_text_arabic(text, max_len=5)\n",
    "            term=tokenize_text_arabic(term, max_len=1)\n",
    "            for token in text:\n",
    "                if token not in text_vocab:\n",
    "                    text_vocab[token] = text_num_embeddings\n",
    "                    text_num_embeddings += 1\n",
    "            text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text]\n",
    "            text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "            text_token_list.append(text)\n",
    "            text_outputs_emp.append(text_input_emb)\n",
    "            unk_token = \"<unk>\"\n",
    "            term_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            term_num_embeddings = 1\n",
    "            for token in term:\n",
    "                if token not in term_vocab:\n",
    "                    term_vocab[token] = term_num_embeddings\n",
    "                    term_num_embeddings += 1\n",
    "            term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term]\n",
    "            term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "            term_token_list.append(term)\n",
    "            term_outputs_emp.append(term_input_emb)\n",
    "        # Call the model forward method with concatenated tensors\n",
    "        logits = model(term_outputs_emp, text_outputs_emp, text_token_list, term_token_list)\n",
    "        # Compute logits and loss\n",
    "        train_polarity = Polarity\n",
    "        if(logits.shape[0] == train_polarity.shape[0]) :\n",
    "          loss = criterion(logits, train_polarity.long())\n",
    "          # Backpropagation and weight updates\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          running_loss += loss.item()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for text_outputs , term_outputs , Polarity in val_dataloader:\n",
    "          text_token_list = []\n",
    "          term_token_list = []\n",
    "          term_outputs_emp = []\n",
    "          text_outputs_emp = []\n",
    "          for i in range(len(text_outputs)):\n",
    "            text = text_outputs[i]\n",
    "            term = term_outputs[i]\n",
    "            unk_token = \"<unk>\"\n",
    "            text_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            text_num_embeddings = 1\n",
    "            text= tokenize_text_arabic(text, max_len=5)\n",
    "            term=tokenize_text_arabic(term, max_len=1)\n",
    "            for token in text:\n",
    "                if token not in text_vocab:\n",
    "                    text_vocab[token] = text_num_embeddings\n",
    "                    text_num_embeddings += 1\n",
    "            text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text]\n",
    "            text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "            text_token_list.append(text)\n",
    "            text_outputs_emp.append(text_input_emb)\n",
    "            unk_token = \"<unk>\"\n",
    "            term_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            term_num_embeddings = 1\n",
    "            for token in term:\n",
    "                if token not in term_vocab:\n",
    "                    term_vocab[token] = term_num_embeddings\n",
    "                    term_num_embeddings += 1\n",
    "            term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term]\n",
    "            term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "            term_token_list.append(term)\n",
    "            term_outputs_emp.append(term_input_emb)\n",
    "          logits = model(term_outputs_emp, text_outputs_emp, text_token_list, term_token_list)\n",
    "          test_polarity = Polarity\n",
    "          if (test_polarity.shape[0] == logits.shape[0]):\n",
    "              _, predicted = torch.max(logits.data, 1)\n",
    "              total += test_polarity.size(0)\n",
    "              correct += (predicted == test_polarity).sum().item()\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {running_loss/len(train_dataloader):.3f}, Validation Accuracy = {(100*correct/total):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "# Initialize variables for accuracy computation\n",
    "y_label = []\n",
    "y_predict = []\n",
    "correct = 0\n",
    "total = 0\n",
    "# Disable gradient computation for testing\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Loop through batches in dataloader\n",
    "    for text_outputs, term_outputs, polarity in test_dataloader:\n",
    "\n",
    "        text_token_list = []\n",
    "        term_token_list = []\n",
    "        term_outputs_emp = []\n",
    "        text_outputs_emp = []\n",
    "        for i in range(len(text_outputs)):\n",
    "            text = text_outputs[i]\n",
    "            term = term_outputs[i]\n",
    "            unk_token = \"<unk>\"\n",
    "            text_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            text_num_embeddings = 1\n",
    "            text= tokenize_text_arabic(text, max_len=5)\n",
    "            term=tokenize_text_arabic(term, max_len=1)\n",
    "            for token in text:\n",
    "                if token not in text_vocab:\n",
    "                    text_vocab[token] = text_num_embeddings\n",
    "                    text_num_embeddings += 1\n",
    "            text_word_emb = nn.Embedding(num_embeddings=len(text_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            text_input = [text_vocab[token] if token in text_vocab else text_vocab[unk_token] for token in text]\n",
    "            text_input_emb = text_word_emb(torch.LongTensor(text_input))\n",
    "            text_token_list.append(text)\n",
    "            text_outputs_emp.append(text_input_emb)\n",
    "            unk_token = \"<unk>\"\n",
    "            term_vocab = {unk_token: 0} # initialize with <unk> token\n",
    "            term_num_embeddings = 1\n",
    "            for token in term:\n",
    "                if token not in term_vocab:\n",
    "                    term_vocab[token] = term_num_embeddings\n",
    "                    term_num_embeddings += 1\n",
    "            term_word_emb = nn.Embedding(num_embeddings=len(term_vocab), embedding_dim=300)\n",
    "            # Map out-of-vocabulary tokens to <unk> token\n",
    "            term_input = [term_vocab[token] if token in term_vocab else term_vocab[unk_token] for token in term]\n",
    "            term_input_emb = term_word_emb(torch.LongTensor(term_input))\n",
    "            term_token_list.append(term)\n",
    "            term_outputs_emp.append(term_input_emb)\n",
    "        logits = model(term_outputs_emp, text_outputs_emp, text_token_list, term_token_list)\n",
    "        test_polarity = polarity\n",
    "        if (test_polarity.shape[0] == logits.shape[0]):\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += test_polarity.size(0)\n",
    "            correct += (predicted == test_polarity).sum().item()\n",
    "            y_label.extend(test_polarity)\n",
    "            y_predict.extend(predicted)\n",
    "# Print accuracy\n",
    "print(f\"Test Accuracy = {(100*correct/total):.2f}%\")\n",
    "plot_labels = ['positive', 'negative', 'neutral']\n",
    "report = classification_report(y_label, y_predict, target_names=plot_labels)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04af6261560c4189b3862040a8776963": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b44ef20a25c48f59fef9fb55d0a39c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0cbf2bd2b2544ded98b37cf9b4047df4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5c6e875df934b79a58592ca513af567",
      "placeholder": "",
      "style": "IPY_MODEL_25bfb7f8358142168d60b470ff50a8a2",
      "value": " 3.69M/3.69M [00:01&lt;00:00, 3.47MB/s]"
     }
    },
    "0e6cd56e921145d8a63a6a1a6bbf9b58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9619f9a53ac14bba8e4c42849b0f31f5",
      "placeholder": "",
      "style": "IPY_MODEL_553d703c39ac41ec81d500748420dcdf",
      "value": " 367k/? [00:00&lt;00:00, 4.83MB/s]"
     }
    },
    "10bbfa3d58a94a0e869d1eb5afd932a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e6326c246704e2c93893ec3d86e7af6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39f21d5bca404d0084bca71c7e972bbc",
       "IPY_MODEL_cd51365f17f5424b9a9b0154c080b04e",
       "IPY_MODEL_76fc284d052146b386dcd4efed282e07"
      ],
      "layout": "IPY_MODEL_273dcb00e48e48b99a88f46f7a5a2719"
     }
    },
    "205141d6a1d6461bb20987da88f69821": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25bfb7f8358142168d60b470ff50a8a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "273dcb00e48e48b99a88f46f7a5a2719": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "290d30dd0a6b41518b88567d96adf946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39f21d5bca404d0084bca71c7e972bbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d45887145cd43238a642236c6000d5d",
      "placeholder": "",
      "style": "IPY_MODEL_e8b954662f86476e9612da0b7b03a54a",
      "value": "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.6.0/models/ner/aqmar_charlm.pt: 100%"
     }
    },
    "3d216e5572544c40afe8ef26b25ae4a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42d528f181904ef6a828b7cc96dd5b8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "553d703c39ac41ec81d500748420dcdf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57cebdadd6324d8b89a1188fdbb961c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ce6b65e37824468b3e96b001fe2bf3d",
      "placeholder": "",
      "style": "IPY_MODEL_290d30dd0a6b41518b88567d96adf946",
      "value": " 123M/123M [00:07&lt;00:00, 20.2MB/s]"
     }
    },
    "57d83a9fd09f46bf9c00dfc10f375551": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "596199bcd9ee4e36871cfb1737127689": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59c8993e16dc4653954b9db8fafd1599": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fbafd759813473b868337eae29efed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f1e3e441cc74f659e117fc3155352b1",
       "IPY_MODEL_61271eca148d482499082061bf6e8b12",
       "IPY_MODEL_0e6cd56e921145d8a63a6a1a6bbf9b58"
      ],
      "layout": "IPY_MODEL_72c9359b6ec54eb7ad6b9d8b152dcbcf"
     }
    },
    "61271eca148d482499082061bf6e8b12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4228f7315ae49839817330da67b5007",
      "max": 45744,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b44ef20a25c48f59fef9fb55d0a39c3",
      "value": 45744
     }
    },
    "6ad535cd41b740978ea1adc60fe75bb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0e70b43a3fa401ea22aeda3f76656d6",
      "max": 3693437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de4ca1411d3048b9aef817dc818ac451",
      "value": 3693437
     }
    },
    "72c9359b6ec54eb7ad6b9d8b152dcbcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "739ea661a13546b598dff3ae08dcf373": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76fc284d052146b386dcd4efed282e07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d216e5572544c40afe8ef26b25ae4a9",
      "placeholder": "",
      "style": "IPY_MODEL_596199bcd9ee4e36871cfb1737127689",
      "value": " 64.3M/64.3M [00:04&lt;00:00, 19.6MB/s]"
     }
    },
    "7d45887145cd43238a642236c6000d5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d9a6ba8a22a4d049d84973eab7ead0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f1e3e441cc74f659e117fc3155352b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9891a8dd14a49a98912d9dd55cd666f",
      "placeholder": "",
      "style": "IPY_MODEL_205141d6a1d6461bb20987da88f69821",
      "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: "
     }
    },
    "8246ced5b5cf4df381e2d6af92bfcbf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ce6b65e37824468b3e96b001fe2bf3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9619f9a53ac14bba8e4c42849b0f31f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5c6e875df934b79a58592ca513af567": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8691aa66c644cd2b2b1d3522c5568d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb96c9e07fda4fcc97701519586c04c6",
      "placeholder": "",
      "style": "IPY_MODEL_ea5378c90cfc450486ae17bf217b45fc",
      "value": "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.6.0/models/lemma/padt_nocharlm.pt: 100%"
     }
    },
    "b64ca2b90ae94751b7caddd4be6d88c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59c8993e16dc4653954b9db8fafd1599",
      "max": 123007708,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8246ced5b5cf4df381e2d6af92bfcbf5",
      "value": 123007708
     }
    },
    "cd51365f17f5424b9a9b0154c080b04e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d9a6ba8a22a4d049d84973eab7ead0e",
      "max": 64272521,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57d83a9fd09f46bf9c00dfc10f375551",
      "value": 64272521
     }
    },
    "d052683eb8424ce5a87ea18e0b117311": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8691aa66c644cd2b2b1d3522c5568d4",
       "IPY_MODEL_6ad535cd41b740978ea1adc60fe75bb6",
       "IPY_MODEL_0cbf2bd2b2544ded98b37cf9b4047df4"
      ],
      "layout": "IPY_MODEL_04af6261560c4189b3862040a8776963"
     }
    },
    "d4228f7315ae49839817330da67b5007": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de4ca1411d3048b9aef817dc818ac451": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1b81c4aea904c96a8171b78ede718f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_739ea661a13546b598dff3ae08dcf373",
      "placeholder": "",
      "style": "IPY_MODEL_42d528f181904ef6a828b7cc96dd5b8e",
      "value": "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.6.0/models/pretrain/fasttextwiki.pt: 100%"
     }
    },
    "e6eb157a90474e348abc20c0c5ff4b53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e1b81c4aea904c96a8171b78ede718f8",
       "IPY_MODEL_b64ca2b90ae94751b7caddd4be6d88c5",
       "IPY_MODEL_57cebdadd6324d8b89a1188fdbb961c7"
      ],
      "layout": "IPY_MODEL_10bbfa3d58a94a0e869d1eb5afd932a8"
     }
    },
    "e8b954662f86476e9612da0b7b03a54a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9891a8dd14a49a98912d9dd55cd666f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea5378c90cfc450486ae17bf217b45fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb96c9e07fda4fcc97701519586c04c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0e70b43a3fa401ea22aeda3f76656d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
