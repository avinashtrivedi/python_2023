{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2E5kRk-evfk"
   },
   "source": [
    "# Using pre-trained word embeddings with CNN\n",
    "\n",
    "This code is taken from KERAS.IO\n",
    "\n",
    "The original file:\n",
    "https://keras.io/examples/nlp/pretrained_word_embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW2gDOveevfp"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqMCtKB_evfp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTWGhrSdevfr"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we show how to train a text classification model that uses pre-trained\n",
    "word embeddings.\n",
    "\n",
    "We'll work with the Newsgroup20 dataset, a set of 20,000 message board messages\n",
    "belonging to 20 different topic categories.\n",
    "\n",
    "For the pre-trained word embeddings, we'll use\n",
    "[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzSyJ7mJevfr"
   },
   "source": [
    "## Download the Newsgroup20 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cKYTKxdevfs",
    "outputId": "b22f901c-48ab-442e-c44e-f6f5130c108f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
      "17329808/17329808 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr1JGx2uevfs"
   },
   "source": [
    "## Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXxnlGHbevft",
    "outputId": "b86979f1-0dfb-4180-e37e-687e552f70d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['comp.windows.x', 'sci.electronics', 'soc.religion.christian', 'sci.crypt', 'comp.graphics', 'sci.space', 'comp.sys.ibm.pc.hardware', 'rec.sport.baseball', 'misc.forsale', 'talk.politics.mideast', 'rec.motorcycles', 'talk.politics.misc', 'comp.os.ms-windows.misc', 'talk.politics.guns', 'talk.religion.misc', 'alt.atheism', 'rec.autos', 'rec.sport.hockey', 'comp.sys.mac.hardware', 'sci.med']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['38645', '38361', '38486', '38849', '38433']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Plk0RtXLevft"
   },
   "source": [
    "Here's a example of what one file contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vE7dBZ6evfu",
    "outputId": "940dfa2e-5f78-4fbc-8b50-b7a8fa9d1f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7THgWzMevfu"
   },
   "source": [
    "As you can see, there are header lines that are leaking the file's category, either\n",
    "explicitly (the first line is literally the category name), or implicitly, e.g. via the\n",
    "`Organization` filed. Let's get rid of the headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKcApOQRevfv"
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3CySSBZevfv"
   },
   "source": [
    "There's actually one category that doesn't have the expected number of files, but the\n",
    "difference is small enough that the problem remains a balanced classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95TgXjVX-Wm-",
    "outputId": "7c126579-b52e-4cbf-c075-846cf95fda4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 19997)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN2KFw5aevfv"
   },
   "source": [
    "## Shuffle and split the data into training & validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1zjPMQ7evfw"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCBaVgm1b_M0",
    "outputId": "8f021700-0bc5-469a-c6b8-729b43c9afd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In article <0P6a3B1w165w@cybernet.cse.fau.edu> jimg@cybernet.cse.fau.edu (Jim Gorycki) writes:\\n>\\n>A little Bio from _Sun-Sentinel_\\n>Torrey, the architect of four consecutive Stanley Cup champions as \\n>persident and general manager of the New York Islanders.\\n>Throughout his 27 years in the NHL, Bill Torrey's bow ties have become\\n>as much of a signature as Andre Agassi's hair.\\n>\\n>The Panthers will introduce a uniform, insignia, and ticket-price \\n>information in early next month.  In the meantime, Huizenga leaves the\\n>day-to-day operation in the hands of Torrey and Bob Clarke, the VP and\\n>GM.\\n>\\n\\nThe San Jose Sharks and Ottawa Senators are each on their second GM\\nalready...I'd be willing to wager that both the Sharks and Senators\\nwill probably see their 3rd GM's and perhaps their 4th, before we\\nsee the Panthers second.\\n\\nGerald\\n\",\n",
       " 'Approved: christian@aramis.rutgers.edu\\n\\nIn article <May.14.02.11.19.1993.25177@athos.rutgers.edu> seanna@bnr.ca (Seanna (S.M.) Watson) writes:\\n>In article <May.9.05.39.52.1993.27456@athos.rutgers.edu> jhpb@sarto.budd-lake.nj.us (Joseph H. Buehler) writes:\\n>[referring to Mary]\\n>>She was immaculately conceived, and so never subject to Original Sin,\\n>>but also never committed a personal sin in her whole life.  This was\\n>>possible because of the special degree of grace granted to her by God.\\n> skipping......\\n>I don\\'t particularly object to the idea of the assumption, or the\\n>perpetual virginity (both of which I regard as Catholic dogma about \\n>which I will agree to disagree with my Catholic brothers and sisters \\n>in Christ), and I even believe in the virgin birth of Jesus, but \\n>this concept of Mary\\'s sinlessness seems to me to be at odds with \\n>the rest of Christian doctrine as I understand it.\\n\\nThe Catholic church has an entirely different view of Mary than do \\n\"most\" other Christian churches (those with parallel beliefs\\nnotwithstanding).  Christ, by most accounts, is the only sinless\\nperson to ever live.  I too, have trouble with a sinless Mary\\nconcept just. \\nAs for the related issue of the \"original\" sin - only Adam and\\nEve will answer for that one.  My children do not answer for my sins,\\ncertainly I only answer for mine.\\n--\\nLarry Autry\\nSilicon Graphics, St. Louis\\nautry@sgi.com \\n',\n",
       " \"Lines: 39\\n\\nIn article <C5r4IA.A21@acsu.buffalo.edu> v111qheg@ubvmsb.cc.buffalo.edu (P.VASILION) writes:\\n>\\n>\\tWoa, little brain trust. Brian, this who thing is not about someone\\n>thinking they are THE SECOND COMMING. Its about **YOUR** civil rights. Would\\n>you want the FEDS to come marching into your home with a warrant that probably\\n>wouldn't stand up in court, arrest you and your family after attempting to kill\\n>you and haul you off to jail without due process? This is what has happened \\n>in Texas. With the Davidians all dead, no one will know the truth - only\\n>what the White House wants you to think. Government does not exist for you!\\n>Government exists for itself and will do what ever it needs to preserve \\n>itself.\\n\\nProbably not.  But then, I don't pack heavy weaponry with intent to use it.\\nYou don't really think he should have been allowed to keep that stuff do \\nyou?  If so, tell me where you live so I can be sure to steer well clear.\\nThe public also has rights, and they should be placed above those of the\\nindividual.  Go ahead, call me a commie, but you'd be singing a different\\ntune if I exercised my right to rape your daughter.  He broke the law, he\\nwas a threat to society, they did there job - simple.\\n \\n>\\tSupport your First, Second, Fourth, Fifth, Sixth, and Eighth\\n>Amendment rights, lest they be taken away from you just as the FBI did\\n>to the Davidians. Think about it.\\n\\nI'll support them all (except no. 2)\\n \\n>Peter Vasilion, kb2nmv\\n><<STD DISCLAIMERS>>\\n\\n-Tim\\n ______________________________________________________________________________\\n|\\t\\t\\t\\t|\\t\\t\\t\\t       \\t       |\\n|       Timothy J. Brent        |   A man will come to know true happiness,    |\\n|   BRENT@bank.ecn.purdue.edu   |   only when he accepts that he is but a      |\\n|=========$$$$==================|   small part of an infinite universe.\\t       |\\n|       PURDUE UNIVERSITY       |\\t\\t\\t  \\t   -Spinoza    |\\n| MATERIALS SCIENCE ENGINEERING |\\t\\t\\t \\t [paraphrased] |\\n|_______________________________|______________________________________________|\\n________________________________________________________________________________\\n\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzsBsQuIevfw"
   },
   "source": [
    "## Create a vocabulary index\n",
    "\n",
    "Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n",
    "Later, we'll use the same layer instance to vectorize the samples.\n",
    "\n",
    "Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n",
    "be actually 200 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH9ZkV6Kevfw"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_8tXL-aevfx"
   },
   "source": [
    "You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n",
    "print the top 5 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MawEWABTevfx",
    "outputId": "6be965d1-dc08-478c-974a-2097a36fec22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " 'and',\n",
       " 'in',\n",
       " 'is',\n",
       " 'i',\n",
       " 'that',\n",
       " 'it',\n",
       " 'for',\n",
       " 'you',\n",
       " 'this',\n",
       " 'on']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cD_DchIMZMu7",
    "outputId": "30e676ab-6d55-4242-85fa-62ffe2d53f81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of voc\n",
    "len(vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKsV2c20evfx"
   },
   "source": [
    "Let's vectorize a test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dffpv5Xxevfy",
    "outputId": "762d48fb-f9ef-4c4c-d6e5-54bfd3b3c28b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3649, 1765,   15,    2, 6025])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vectorizer([[\"the cat sat on the mat\"]] )\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_BGqQuPZTIj"
   },
   "outputs": [],
   "source": [
    "output.numpy()[0, :16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzbYvBclevfy"
   },
   "source": [
    "As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n",
    "word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n",
    "reserved for \"out of vocabulary\" tokens.\n",
    "\n",
    "Here's a dict mapping words to their indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AdOTYNfevfy"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkwPFOOCevfy"
   },
   "source": [
    "As you can see, we obtain the same encoding as above for our test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2rHmNyvevfz",
    "outputId": "d6ac9906-2a44-41e1-9f97-7a778a5789d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3823, 1745, 15, 2, 7459]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E62nJCUdevfz"
   },
   "source": [
    "## Load pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3TqqCtuevfz"
   },
   "source": [
    "Let's download pre-trained GloVe embeddings (a 822M zip file).\n",
    "\n",
    "You'll need to run the following commands:\n",
    "\n",
    "```\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHJ6lrivevfz"
   },
   "source": [
    "The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
    "100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
    "\n",
    "Let's make a dict mapping words (strings) to their NumPy vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfaEcdYLZzCc",
    "outputId": "3de94d2a-18d8-4a8f-8657-d354dc0cb3ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-17 01:36:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-02-17 01:36:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-02-17 01:36:37--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
      "\n",
      "2023-02-17 01:39:16 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRsbqbpGainz",
    "outputId": "386e7ab5-0ffc-4def-bdee-62646368f79a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config',\n",
       " 'glove.6B.300d.txt',\n",
       " 'glove.6B.200d.txt',\n",
       " 'glove.6B.100d.txt',\n",
       " 'glove.6B.50d.txt',\n",
       " 'glove.6B.zip',\n",
       " 'sample_data']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bra03s-payJ6",
    "outputId": "cf710900-7911-4ed0-8e15-9ac4155b1ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "#!head -10 glove.6B.50d.txt\n",
    "!wc -l glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpWeop-Fevfz",
    "outputId": "413a4cc7-99bd-4c38-b2c7-e1b0befaf1d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file =\"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dt8jCODha81G",
    "outputId": "a490a9b6-2834-490f-dbd8-582fac1b9eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOt5ivHCevf0"
   },
   "source": [
    "Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
    "`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
    "vector for the word of index `i` in our `vectorizer`'s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgNQ00_Fevf0",
    "outputId": "1e5a8409-0532-4497-9293-76448ec9ea42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18005 words (1995 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyePhjtGevf0"
   },
   "source": [
    "Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n",
    "\n",
    "Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n",
    "update them during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7lcI_Scevf0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue77DyMyevf0"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "A simple 1D convnet with global max pooling and a classifier at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxlOiVI8evf1",
    "outputId": "d12d7a78-a6d1-4795-e7be-72b3b252dc13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         2000200   \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, None, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,247,516\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB_9OYtvevf1"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n",
    "are right-padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_ilfygwevf1"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9K-1cXkcSVn",
    "outputId": "19a90bf4-a29d-449b-e5d6-9af5d3ff46e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15998, 200), (15998,), (3999, 200), (3999,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEzqvQaSevf1"
   },
   "source": [
    "We use categorical crossentropy as our loss since we're doing softmax classification.\n",
    "Moreover, we use `sparse_categorical_crossentropy` since our labels are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7uLQuWncnOK",
    "outputId": "7344eade-3cb0-437a-9ac0-d26f87a0f4d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 15, 18,  5, 15])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQOZQYvNDaPS",
    "outputId": "c5b0289e-3ff7-4c18-b66b-dd0d0a7111c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    4,   476,  8412,  8670,     2,   200,   688,     3,    16,\n",
       "           7,  2638,   195,    23,   261,    33,     9,   118,     2,\n",
       "        6489,   200,     3,    39,     8,     3,  1453,  8313,    28,\n",
       "        7949,  2834,  1828,     2,   694,     6,     7,    14,     2,\n",
       "        2834,   906,    18,    21,  8493,    21,  5036,  5700,     3,\n",
       "        7619,   214,    20,  2992,    11,    78,   688,     3,    17,\n",
       "        6833,    12,  3049, 11951,     3,     2,  2573,    21,    90,\n",
       "          21,    63,   728,     7,    65,   509,     9,   118,     3,\n",
       "        1541,   761,  8670, 15858,    35,    16,     3,   247,   496,\n",
       "        1952, 17124,  1384,  2483,  1195,    23,    13,    64,     4,\n",
       "         109,     5,  6489,   200,    10,     8,     4,  1014,   533,\n",
       "         138,  1402,  2327,   517,     1,   287, 14699,   173,  6010,\n",
       "           1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkQdtAZJevf1"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "hist=model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuW1ZCNBdFed",
    "outputId": "c8e57212-bc10-447a-8df6-978fe60cf9a8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.651577949523926,\n",
       " 1.9744912385940552,\n",
       " 1.531005620956421,\n",
       " 1.2855956554412842,\n",
       " 1.1128093004226685,\n",
       " 0.990588903427124,\n",
       " 0.8745663166046143,\n",
       " 0.776181161403656,\n",
       " 0.6794454455375671,\n",
       " 0.602085530757904,\n",
       " 0.5314925312995911,\n",
       " 0.45894214510917664,\n",
       " 0.39452025294303894,\n",
       " 0.35732564330101013,\n",
       " 0.307815819978714,\n",
       " 0.27414336800575256,\n",
       " 0.25425970554351807,\n",
       " 0.22366692125797272,\n",
       " 0.2113034725189209,\n",
       " 0.1925293505191803]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwPC-BFYv5J5"
   },
   "source": [
    "## TASKS\n",
    "\n",
    "A) **Inference**\n",
    "\n",
    "* So far, we learned how to train a model.  But we do not discuss how to apply it for a new textual instance. We should put the capabilities learned during training to work. We call inference that is applying a machine learning model to a dataset and generating an output or prediction.\n",
    "\n",
    "So write a function **inference(model, textual_input)** to do inference for the CNN based model trained above \"pre-trained word embeddings with CNN\"\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZrCuLPS0wQ_0"
   },
   "outputs": [],
   "source": [
    "def inference(model, textual_input):\n",
    "  # your codes here\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zau7XDnCU6y0"
   },
   "source": [
    "B) **GRADIO** \n",
    "\n",
    "Take a look at gradio and build a demo for your model with a user-friendly web interface so that we can use it. \n",
    "\n",
    "https://gradio.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vdSTTwnfVSYO"
   },
   "outputs": [],
   "source": [
    "# your gradio code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJ4IL9deVcKq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
