{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3070032",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatentDirichletAllocation\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[0;32m     15\u001b[0m sid \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# No of Topic\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\__init__.py:146\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\__init__.py:155\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    158\u001b[0m     ChunkScore,\n\u001b[0;32m    159\u001b[0m     accuracy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     tree2conlltags,\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\api.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunk parsing API\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m##  Chunk Parser Interface\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserI\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\util.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_tag\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m str2tuple\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tree\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m## EVALUATION\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(chunker, gold):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree\\__init__.py:20\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mNLTK Tree Package\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03mThis package may be used for representing hierarchical language\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03mstructures, such as syntax trees and morphological trees.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# TODO: add LabelledTree (can be used for dependency trees)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimmutable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     ImmutableMultiParentedTree,\n\u001b[0;32m     22\u001b[0m     ImmutableParentedTree,\n\u001b[0;32m     23\u001b[0m     ImmutableProbabilisticTree,\n\u001b[0;32m     24\u001b[0m     ImmutableTree,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparented\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiParentedTree, ParentedTree\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bracket_parse, sinica_parse\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "from spacy.language import Language\n",
    "pipeline = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# No of Topic\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a188f",
   "metadata": {},
   "source": [
    "# Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Netflix_trailers = pd.read_csv('df_Netflix_trailers.csv')\n",
    "df_Netflix_highlights = pd.read_csv('df_Netflix_highlights.csv')\n",
    "df_Formula1_trailers = pd.read_csv('df_Formula1_trailers.csv')\n",
    "df_Formula1_highlights = pd.read_csv('df_Formula1_highlights.csv')\n",
    "df_Miscellaneous = pd.read_csv('df_Miscellaneous.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c962af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Netflix_trailers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52128a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Netflix_highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64878592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Formula1_trailers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Formula1_highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde22cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc4b97",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "\n",
    "replace = [\n",
    "    (r\"<a[^>]*>(.*?)</a>\", r\"\\1\"),  # Matches most URLs\n",
    "    (r\"(?<=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n",
    "    (r\"\\d+\", \"number\"),              # Map digits to special token <numbr>\n",
    "    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n",
    "    (r\"\\s+\", \" \"),                   # Stips extra whitespace\n",
    "    (r\"[<br>]+\", \" \") ,\n",
    "    (r\"[#&]+\", \" \"),\n",
    "]\n",
    "    \n",
    "@Language.component(\"preprocesser\")\n",
    "def ng20_preprocess(doc):\n",
    "    tokens = [token for token in doc \n",
    "              if not any((token.is_stop, token.is_punct))]\n",
    "    tokens = [token.lemma_.lower().strip() for token in tokens]\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "pipeline.add_pipe(\"preprocesser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e50a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_data(data):\n",
    "    train_sentences = []\n",
    "\n",
    "    for i, d in enumerate(data['textDisplay']):\n",
    "        for repl in replace:\n",
    "            d = re.sub(repl[0], repl[1], d)\n",
    "        train_sentences.append(d)\n",
    "\n",
    "    docs = []\n",
    "    for sent in tqdm(train_sentences):\n",
    "        docs.append(pipeline(sent))\n",
    "    \n",
    "    vocab_size = len(set(\" \".join(docs).split(\" \")))\n",
    "\n",
    "    return docs,vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d2a46",
   "metadata": {},
   "source": [
    "# topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis(docs,vocab_size):\n",
    "    # bow_featurizer = CountVectorizer(max_features=vocab_size, max_df=0.95, min_df=0.005, stop_words='english')\n",
    "    tfidf_featurizer = TfidfVectorizer(max_features=vocab_size, max_df=0.95, stop_words='english')\n",
    "\n",
    "    # X_bow = bow_featurizer.fit_transform(docs)\n",
    "    X_tfidf = tfidf_featurizer.fit_transform(docs)\n",
    "    feature_names = tfidf_featurizer.get_feature_names_out()\n",
    "    X = X_tfidf.toarray()\n",
    "    \n",
    "    return X,feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba89226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    plt.clf()\n",
    "    cols = 5\n",
    "    rows = k // 5 + k % 5\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[::-1][:n_top_words]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx + 1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939418fb",
   "metadata": {},
   "source": [
    "# sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c183596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sent):\n",
    "    ss = sid.polarity_scores(sent)\n",
    "    sentiment = sorted(ss.items(),key=lambda x:x[1])[-1][0]\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95309d96",
   "metadata": {},
   "source": [
    "# Analysis for df_Netflix_trailers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_plot(data):\n",
    "    docs, vocab_size = get_cleaned_data(data)\n",
    "    data['Clean_text'] = docs\n",
    "    data['Sentiment'] = data['Clean_text'].progress_apply(get_sentiment)\n",
    "    X,feature_names =  Analysis(docs,vocab_size)\n",
    "    lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "    lda.fit(X)\n",
    "    plot_top_words(lda, feature_names, 10, 'LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74fff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_plot(df_Netflix_trailers)\n",
    "df_Netflix_trailers.to_csv('df_Netflix_trailers_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb0af3",
   "metadata": {},
   "source": [
    "# df_Netflix_highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_plot(df_Netflix_highlights)\n",
    "df_Netflix_highlights.to_csv('df_Netflix_highlights_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33dcd0",
   "metadata": {},
   "source": [
    "# df_Formula1_trailers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04032926",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_plot(df_Formula1_trailers)\n",
    "df_Formula1_trailers.to_csv('df_Formula1_trailers_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5b182",
   "metadata": {},
   "source": [
    "# df_Formula1_highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_plot(df_Formula1_highlights)\n",
    "df_Formula1_highlights.to_csv('df_Formula1_highlights.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563e3fb",
   "metadata": {},
   "source": [
    "# df_Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_plot(df_Miscellaneous)\n",
    "df_Miscellaneous.to_csv('df_Miscellaneous_cleaned.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
