{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2E5kRk-evfk"
   },
   "source": [
    "# Using pre-trained word embeddings with CNN\n",
    "\n",
    "This code is taken from KERAS.IO\n",
    "\n",
    "The original file:\n",
    "https://keras.io/examples/nlp/pretrained_word_embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW2gDOveevfp"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqMCtKB_evfp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTWGhrSdevfr"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we show how to train a text classification model that uses pre-trained\n",
    "word embeddings.\n",
    "\n",
    "We'll work with the Newsgroup20 dataset, a set of 20,000 message board messages\n",
    "belonging to 20 different topic categories.\n",
    "\n",
    "For the pre-trained word embeddings, we'll use\n",
    "[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzSyJ7mJevfr"
   },
   "source": [
    "## Download the Newsgroup20 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cKYTKxdevfs",
    "outputId": "cf0a4648-9290-420c-a201-b59e1edf4e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
      "17329808/17329808 [==============================] - 12s 1us/step\n"
     ]
    }
   ],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr1JGx2uevfs"
   },
   "source": [
    "## Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXxnlGHbevft",
    "outputId": "3b20d4b8-5edc-4302-9e37-5261c9cb162e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['comp.windows.x', 'misc.forsale', 'sci.crypt', 'sci.space', 'comp.sys.ibm.pc.hardware', 'soc.religion.christian', 'alt.atheism', 'rec.sport.baseball', 'talk.politics.mideast', 'comp.graphics', 'rec.sport.hockey', 'comp.os.ms-windows.misc', 'rec.motorcycles', 'talk.religion.misc', 'sci.electronics', 'sci.med', 'talk.politics.misc', 'rec.autos', 'talk.politics.guns', 'comp.sys.mac.hardware']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['38998', '38375', '39664', '38337', '38315']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Plk0RtXLevft"
   },
   "source": [
    "Here's a example of what one file contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vE7dBZ6evfu",
    "outputId": "d5c7ebb2-ae19-420f-cdb6-b5b2954acdcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7THgWzMevfu"
   },
   "source": [
    "As you can see, there are header lines that are leaking the file's category, either\n",
    "explicitly (the first line is literally the category name), or implicitly, e.g. via the\n",
    "`Organization` filed. Let's get rid of the headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKcApOQRevfv",
    "outputId": "b5eec63d-46f8-4754-86d6-f097b3e09fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3CySSBZevfv"
   },
   "source": [
    "There's actually one category that doesn't have the expected number of files, but the\n",
    "difference is small enough that the problem remains a balanced classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95TgXjVX-Wm-",
    "outputId": "7b1bf68f-2af0-44b0-9771-f3d5ca08773d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 19997)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN2KFw5aevfv"
   },
   "source": [
    "## Shuffle and split the data into training & validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1zjPMQ7evfw"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCBaVgm1b_M0",
    "outputId": "90d0bf69-36bc-4775-cd70-66dc28655d22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nIn article <1993Apr21.040956.12823@wuecl.wustl.edu>\\nmvs1@cec2.wustl.edu (Michael Virata Sy) writes:\\n \\n>\\n>        Don't forget Paul Ysebaert, ex-Devil.  He's a good team player.\\n \\n      And Dino Ciccarelli and Ray Sheppard and so on and so on.....\\n \\n    : )\\n \\nLaurie Marshall\\nWayne State University\\nDetroit, Michigan\\nGo Wings!!!!\\n\",\n",
       " 'Approved: christian@aramis.rutgers.edu\\n\\nIn article <Apr.21.03.25.41.1993.1322@geneva.rutgers.edu>  \\nJBUDDENBERG@vax.cns.muskingum.edu (Jimmy Buddenberg) writes:\\n> \\n> Hello all.  We are doing a bible study (at my college) on Revelations.  We\\n> have been doing pretty good as far as getting some sort of reasonable\\n> interpretation.  We are now on chapters 17 and 18 which talk about the\\n> woman on the beast and the fall of Babylon.  I believe the beast is the\\n> Antichrist (some may differ but it seems obvious) and the woman represents\\n> Babylon which stands for Rome or the Roman Catholic Church.  What are some\\n> views on this interpretation?  Is the falling Babylon in chapter 18 the same\\n> Babylon in as in chapter 17?  The Catholic church?\\n> Hate to step on toes.\\n> thanks\\n\\nAn interesting interpretation of Revelation 17 and 18 has been given by  \\nevangelist David Wilkerson.  I am not saying that I totally agree with his  \\ninterpretation, but it is certainly believable and good food for thought.  He  \\ninterprets the Babylon of Revelation 17-18 as being none other than the good  \\nold U. S. of A.  That\\'s right, America.  He supports his claim in several ways.   \\nThe Babylon of Revelation is THE world leader in trade and commerce, and the  \\nWHOLE WORLD wept when Babylon fell.  The American dollar, despite the Japanese  \\nsuccess of the 20th century, is STILL the most sought after currency in the  \\nworld.  If the U.S. were destroyed, wouldn\\'t the whole world mourn?  The bible  \\nalso talks about Babylon being a home of harlots, sin, and adultery (I am  \\nparaphrasing, of course).  Babylon\\'s sin affected, or should I say, infected,  \\nthe whole world.  It doesn\\'t take much looking to see that the U.S. is in a  \\nstate of moral decay.  Hasn\\'t the American culture and Hollywood spread the \"do  \\nit if it feels good\" mentality all over the world.  I think, though, that what  \\nMr. Wilkerson uses as his strongest argument is the fact that Revelation calls  \\nBabylon \"Babylon the Great\" and portrays it as the most powerful nation on  \\nearth.  No matter how dissatisfied you are with the state of our country, I  \\ndon\\'t think you would have too much trouble agreeing that the U.S. is STILL the  \\nmost powerful nation on earth.\\n\\nAgain, this interpretation is not NECESSARILY my own, but I do find it worthy  \\nof consideration.\\n\\nJeffrey Little\\n',\n",
       " \"NNTP-Posting-Host: stein2.u.washington.edu\\n\\ndemon@desire.wright.edu (Not a Boomer) writes:\\n>\\tA judge denied GM's new trial motion, even though GM says it has two\\n>new witnesses that said the occupant of the truck was dead from the impact, not\\n>from the fire.\\n>\\n>\\tThoughts?\\n\\nHow can a witness tell that someone in a burning truck is dead rather than\\nunconscious?\\n\\n>\\tIt's kind of scary when you realize that judges are going to start\\n>denying new trials even when new evidence that contradicts the facts that led\\n>to the previous ruling appear.\\n>\\n>\\tOr has the judge decided that the new witnesses are not to be believed? \\n>Shouldn't that be up to a jury?\\n\\nWhat kind of witnesses?  If we are talking about witnesses who were at\\nthe accident, or were otherwise directly involved (e.g., paramedics,\\nemergency room doctors, etc.), then they should have been used at the\\nfirst trial.  You don't get a new trial because you screwed up and\\nforgot to call all of your witnesses.\\n\\nIf we are talking about new expert witnesses who will offer new\\ninterpretations of the data, note that the loser can *ALWAYS* find\\nsuch witnesses.  If this were grounds for a new trial, then the loser\\ncould *ALWAYS* get a new trial, and keep doing so until the loser\\nbecomes a winner (and then the other side would come up with new\\nexpert witnesses).\\n\\n--Tim Smith\\n\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzsBsQuIevfw"
   },
   "source": [
    "## Create a vocabulary index\n",
    "\n",
    "Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n",
    "Later, we'll use the same layer instance to vectorize the samples.\n",
    "\n",
    "Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n",
    "be actually 200 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH9ZkV6Kevfw"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_8tXL-aevfx"
   },
   "source": [
    "You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n",
    "print the top 5 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MawEWABTevfx",
    "outputId": "bf6d3595-70f1-4d15-c495-43899cd4b2fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " 'and',\n",
       " 'in',\n",
       " 'is',\n",
       " 'i',\n",
       " 'that',\n",
       " 'it',\n",
       " 'for',\n",
       " 'you',\n",
       " 'this',\n",
       " 'on']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cD_DchIMZMu7",
    "outputId": "f4b99c0d-de06-40cf-ca2e-ab655bba41f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of voc\n",
    "len(vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKsV2c20evfx"
   },
   "source": [
    "Let's vectorize a test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dffpv5Xxevfy",
    "outputId": "39df082d-8dab-4994-d877-6b0fa83110e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3520, 1741,   15,    2, 6033])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vectorizer([[\"the cat sat on the mat\"]] )\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_BGqQuPZTIj",
    "outputId": "59c4c494-9d0f-40ae-a99c-59f32d708318"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3520, 1741,   15,    2, 6033,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.numpy()[0, :16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzbYvBclevfy"
   },
   "source": [
    "As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n",
    "word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n",
    "reserved for \"out of vocabulary\" tokens.\n",
    "\n",
    "Here's a dict mapping words to their indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AdOTYNfevfy"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkwPFOOCevfy"
   },
   "source": [
    "As you can see, we obtain the same encoding as above for our test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2rHmNyvevfz",
    "outputId": "ed84ece3-2c66-405f-a4b4-94a0407ebcdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3520, 1741, 15, 2, 6033]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E62nJCUdevfz"
   },
   "source": [
    "## Load pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3TqqCtuevfz"
   },
   "source": [
    "Let's download pre-trained GloVe embeddings (a 822M zip file).\n",
    "\n",
    "You'll need to run the following commands:\n",
    "\n",
    "```\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHJ6lrivevfz"
   },
   "source": [
    "The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
    "100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
    "\n",
    "Let's make a dict mapping words (strings) to their NumPy vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMXczRLovkL9",
    "outputId": "44034aa3-bf6d-49c3-c376-98c6ea3694f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-02 23:49:08--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-03-02 23:49:08--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-03-02 23:49:08--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
      "\n",
      "2023-03-02 23:51:47 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRsbqbpGainz",
    "outputId": "df2b0502-2a92-4ad8-e62c-da05281686a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config',\n",
       " 'glove.6B.200d.txt',\n",
       " 'glove.6B.50d.txt',\n",
       " 'glove.6B.100d.txt',\n",
       " 'glove.6B.zip',\n",
       " 'glove.6B.300d.txt',\n",
       " 'sample_data']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bra03s-payJ6",
    "outputId": "5e5a2c53-2d0a-476a-bf72-ae954d0b2f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "#!head -10 glove.6B.50d.txt\n",
    "!wc -l glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpWeop-Fevfz",
    "outputId": "84d0ccfd-3b48-4546-f9c9-80e1b120880f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file =\"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dt8jCODha81G",
    "outputId": "7287ae02-366d-4f23-c6a3-ec5a6951b246"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOt5ivHCevf0"
   },
   "source": [
    "Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
    "`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
    "vector for the word of index `i` in our `vectorizer`'s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgNQ00_Fevf0",
    "outputId": "70d996ec-35d5-496d-9bd1-395989d229bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17935 words (2065 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyePhjtGevf0"
   },
   "source": [
    "Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n",
    "\n",
    "Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n",
    "update them during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7lcI_Scevf0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue77DyMyevf0"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "A simple 1D convnet with global max pooling and a classifier at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxlOiVI8evf1",
    "outputId": "c0591ffa-33e1-4d1e-cea4-73804f25c06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         2000200   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,247,516\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB_9OYtvevf1"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n",
    "are right-padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_ilfygwevf1"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9K-1cXkcSVn",
    "outputId": "16d232dc-5182-4ad0-f3a9-d16cb1459394"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15998, 200), (15998,), (3999, 200), (3999,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEzqvQaSevf1"
   },
   "source": [
    "We use categorical crossentropy as our loss since we're doing softmax classification.\n",
    "Moreover, we use `sparse_categorical_crossentropy` since our labels are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7uLQuWncnOK",
    "outputId": "382809f6-f3b4-4f93-a86e-73c243b27719"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 15, 18,  5, 15])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQOZQYvNDaPS",
    "outputId": "79c324c0-77d5-4c10-cd64-0db5c7030085"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   75,   492,     7,    48,     1,     1,   558,     1,    40,\n",
       "           1,    23,  6863,     3,     2,   126,   476,  1768,    15,\n",
       "           2,  3405,    57,    83,     5,   407,   201,  6930,    23,\n",
       "           2,   550,   116,    30,  1823,     3,     2,   476,  6396,\n",
       "          87,  9050,    35,   447,    49,   107,    47,  1607, 10583,\n",
       "          47,   407,     1,    14,     8,    17,     5,    90,   341,\n",
       "           9,  4759,    32,  1747,   171,  1213,     9,    34,    19,\n",
       "          74,   311,  4370,    29,    49,   224,    10,    34,    19,\n",
       "        2046,    50,     3,  9050,   436,    91,     9,  1029,  7629,\n",
       "         548,     8,  9649,     9,   722,    13,    19,     5,   918,\n",
       "         201,    17,    35,     4,    93,    19,    41,  1280,  4460,\n",
       "         211,    12,  2148,    13,    65,    49,   456,     7,     2,\n",
       "        1608,     8,   136, 18672,     3,    83,    21,     5, 19643,\n",
       "          18,    13,   148,     3,  1143,    10,     5,   550,  9050,\n",
       "          35,    62,   224,    23,    26,   671,  5260,     4,   893,\n",
       "           3,    62,   236,     9,   116,    10,     5,   571,     4,\n",
       "         908,   409,    54,     7,     2,  1608,     6,    23,   224,\n",
       "          72,   406,    50,    14,   285,     2,   476,    13,  1193,\n",
       "          50,    15,     9,    34,   165,    19,   124,    10,   224,\n",
       "         216,   130,   312,   341,   893,     8,   135,    13,   671,\n",
       "          35,     2,    85,    43,    47,    54,    63,    59,  1608,\n",
       "        3163,  1299,  2564,   527,     1,   205,   370,    16,   124,\n",
       "        5541,    20])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkQdtAZJevf1",
    "outputId": "0752c97e-3430-4120-fcc2-9e21d02ce35d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 26s 195ms/step - loss: 2.6496 - acc: 0.1408 - val_loss: 2.0618 - val_acc: 0.3053\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 1.9539 - acc: 0.3247 - val_loss: 1.5839 - val_acc: 0.4636\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 1.5216 - acc: 0.4694 - val_loss: 1.4073 - val_acc: 0.5149\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 1.2911 - acc: 0.5483 - val_loss: 1.2892 - val_acc: 0.5434\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 1.1346 - acc: 0.6023 - val_loss: 1.0562 - val_acc: 0.6357\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 22s 177ms/step - loss: 1.0014 - acc: 0.6500 - val_loss: 1.0445 - val_acc: 0.6434\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 22s 177ms/step - loss: 0.8930 - acc: 0.6855 - val_loss: 1.0219 - val_acc: 0.6564\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.7847 - acc: 0.7282 - val_loss: 1.0197 - val_acc: 0.6644\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.6920 - acc: 0.7540 - val_loss: 1.0405 - val_acc: 0.6779\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.6124 - acc: 0.7855 - val_loss: 1.0507 - val_acc: 0.6777\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.5320 - acc: 0.8107 - val_loss: 1.2607 - val_acc: 0.6477\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.4622 - acc: 0.8380 - val_loss: 1.0925 - val_acc: 0.6884\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.4077 - acc: 0.8564 - val_loss: 1.1218 - val_acc: 0.6962\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 23s 181ms/step - loss: 0.3635 - acc: 0.8755 - val_loss: 1.2750 - val_acc: 0.6742\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.3251 - acc: 0.8894 - val_loss: 1.1147 - val_acc: 0.7054\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 24s 194ms/step - loss: 0.2792 - acc: 0.9051 - val_loss: 1.7138 - val_acc: 0.6159\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.2557 - acc: 0.9122 - val_loss: 1.2824 - val_acc: 0.7037\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.2380 - acc: 0.9204 - val_loss: 1.2151 - val_acc: 0.7114\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.2056 - acc: 0.9312 - val_loss: 1.3342 - val_acc: 0.7077\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 23s 181ms/step - loss: 0.1930 - acc: 0.9369 - val_loss: 1.3744 - val_acc: 0.7087\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "hist=model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuW1ZCNBdFed",
    "outputId": "3e930c50-c0a5-482b-c99d-2418858f30f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6495535373687744,\n",
       " 1.9539433717727661,\n",
       " 1.5216197967529297,\n",
       " 1.2911415100097656,\n",
       " 1.1345709562301636,\n",
       " 1.001423716545105,\n",
       " 0.8930150866508484,\n",
       " 0.7847270965576172,\n",
       " 0.691979169845581,\n",
       " 0.6123533248901367,\n",
       " 0.5320221781730652,\n",
       " 0.46219775080680847,\n",
       " 0.40768852829933167,\n",
       " 0.3635440468788147,\n",
       " 0.32508111000061035,\n",
       " 0.27918991446495056,\n",
       " 0.255742609500885,\n",
       " 0.23802593350410461,\n",
       " 0.205575093626976,\n",
       " 0.19303257763385773]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwPC-BFYv5J5"
   },
   "source": [
    "# TASKS\n",
    "\n",
    "A) **Inference**\n",
    "\n",
    "* So far, we learned how to train a model.  But we do not discuss how to apply it for a new textual instance. We should put the capabilities learned during training to work. We call inference that is applying a machine learning model to a dataset and generating an output or prediction.\n",
    "\n",
    "So write a function **inference(model, textual_input)** to do inference for the CNN based model trained above \"pre-trained word embeddings with CNN\"\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIWWIvQ447fS"
   },
   "outputs": [],
   "source": [
    "def inference(model, textual_input):\n",
    "    # Vectorize the textual input using the same TextVectorization layer\n",
    "    # that was used during training\n",
    "    vectorized_input = vectorizer(np.array([textual_input])).numpy()\n",
    "\n",
    "    # Use the predict method of the model to generate predictions\n",
    "    prediction = model.predict(vectorized_input)\n",
    "\n",
    "    # Convert the prediction to a human-readable class name\n",
    "    predicted_class = class_names[np.argmax(prediction[0])]\n",
    "\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRlzIV_sohQ-",
    "outputId": "fc1a9dc7-98cd-4ae0-cf19-b794658ad47f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 229ms/step\n",
      "Predicted class: soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# Example usage of inference function\n",
    "\n",
    "# new textual instance-1\n",
    "\n",
    "# In my case, the model predicted the input text \"It doesn\\'t take much looking to see that the U.S. is in a  \\nstate of moral decay.\" \n",
    "# which is one of the class labels in the 20 Newsgroup dataset that the model was trained on\n",
    "\n",
    "textual_input = \"It doesn\\'t take much looking to see that the U.S. is in a  \\nstate of moral decay.\"\n",
    "predicted_class = inference(model, textual_input)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdWn3xEu_jPm",
    "outputId": "45912f6a-0352-4b45-b0a0-138bf6cf4760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "Predicted class: soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# Example usage of inference function\n",
    "\n",
    "# new textual instance-2\n",
    "\n",
    "textual_input = \"\\n>As the moderator noted, I think you mean the Assumption.\"\n",
    "predicted_class = inference(model, textual_input)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtXDLYEX-wVp",
    "outputId": "5c6dcc6d-7c73-429a-95ed-fb056387b64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted class: comp.os.ms-windows.misc\n"
     ]
    }
   ],
   "source": [
    "# Example usage of inference function\n",
    "\n",
    "# new textual instance-3\n",
    "\n",
    "textual_input = \"I am interested in learning more about Natural Language Processing.\"\n",
    "predicted_class = inference(model, textual_input)\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5kSvUv5ylyn",
    "outputId": "006d5705-3b3f-4b93-d9de-cfd11ff86a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted class: misc.forsale\n"
     ]
    }
   ],
   "source": [
    "# Example usage of inference function\n",
    "\n",
    "# new textual instance-4\n",
    "\n",
    "textual_input = \"This is a test document about sports\"\n",
    "predicted_class = inference(model, textual_input)\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xW3PlA60RpY",
    "outputId": "1c51ef70-dbfd-47be-b63a-857eb37e1364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "Input text: I am interested in learning more about Natural Language Processing\n",
      "Predicted class: comp.os.ms-windows.misc\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Input text: You don't get a new trial because you screwed up and\n",
      "forgot to call all of your witnesses.\n",
      "Predicted class: talk.politics.guns\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Input text: I love playing sports and being active\n",
      "Predicted class: comp.graphics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage of inference function\n",
    "\n",
    "# new textual instance-5\n",
    "# I used a new method to define a list of textual inputs to generate predictions for\n",
    "\n",
    "textual_inputs = [\n",
    "    \"I am interested in learning more about Natural Language Processing\",\n",
    "    \"You don't get a new trial because you screwed up and\\nforgot to call all of your witnesses.\",\n",
    "    \"I love playing sports and being active\",\n",
    "]\n",
    "\n",
    "# The predictions for each input \n",
    "for input_text in textual_inputs:\n",
    "    predicted_class = inference(model, input_text)\n",
    "    print(\"Input text:\", input_text)\n",
    "    print(\"Predicted class:\", predicted_class)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zau7XDnCU6y0"
   },
   "source": [
    "B) **GRADIO** \n",
    "\n",
    "Take a look at gradio and build a demo for your model with a user-friendly web interface so that we can use it. \n",
    "\n",
    "https://gradio.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUh8T1cCAmt4",
    "outputId": "31dde14c-d483-4337-8a9b-e467b993bef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gradio\n",
      "  Downloading gradio-3.19.1-py3-none-any.whl (14.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (8.4.0)\n",
      "Collecting websockets>=10.0\n",
      "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.92.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2023.1.0)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.22.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.25.1)\n",
      "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (4.2.2)\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.8.7-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.7/140.7 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiofiles\n",
      "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
      "Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from gradio) (2.1.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.5.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from gradio) (4.5.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1\n",
      "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.4)\n",
      "Collecting starlette<0.26.0,>=0.25.0\n",
      "  Downloading starlette-0.25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting httpcore<0.17.0,>=0.15.0\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.12.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (23.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.26.14)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (2.10)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (8.1.3)\n",
      "Collecting anyio<5.0,>=3.0\n",
      "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (5.12.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->gradio) (1.15.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=4.2.0->gradio) (3.15.0)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4711 sha256=81be88c4210a2299f8949763a9e80aa27ecc912dc5af97d7d1f414ab48354ca6\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: rfc3986, pydub, ffmpy, websockets, uc-micro-py, sniffio, python-multipart, pycryptodome, orjson, mdurl, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, anyio, starlette, mdit-py-plugins, httpcore, httpx, fastapi, gradio\n",
      "Successfully installed aiofiles-23.1.0 anyio-3.6.2 fastapi-0.92.0 ffmpy-0.3.0 gradio-3.19.1 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 linkify-it-py-2.0.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.8.7 pycryptodome-3.17 pydub-0.25.1 python-multipart-0.0.6 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.25.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "vdSTTwnfVSYO",
    "outputId": "6c29fec5-d022-4147-f433-3185c4b176b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "(async (port, path, width, height, cache, element) => {\n",
       "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
       "                            return;\n",
       "                        }\n",
       "                        element.appendChild(document.createTextNode(''));\n",
       "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
       "\n",
       "                        const external_link = document.createElement('div');\n",
       "                        external_link.innerHTML = `\n",
       "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
       "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
       "                                    https://localhost:${port}${path}\n",
       "                                </a>\n",
       "                            </div>\n",
       "                        `;\n",
       "                        element.appendChild(external_link);\n",
       "\n",
       "                        const iframe = document.createElement('iframe');\n",
       "                        iframe.src = new URL(path, url).toString();\n",
       "                        iframe.height = height;\n",
       "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
       "                        iframe.width = width;\n",
       "                        iframe.style.border = 0;\n",
       "                        element.appendChild(iframe);\n",
       "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your gradio code goes here\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Define a function to make predictions with your model\n",
    "def predict_category(text):\n",
    "    prediction = inference(model, text)\n",
    "    return prediction\n",
    "\n",
    "# Define a Gradio interface for your function\n",
    "iface = gr.Interface(\n",
    "    fn=predict_category,\n",
    "    inputs=gr.inputs.Textbox(label=\"Enter text here\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Predicted category\"),\n",
    "    title=\"Text Classification Demo\",\n",
    "    description=\"Enter some text and see which category it belongs to.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhnQWbP_Dh5c"
   },
   "source": [
    "## I came up with a new method to create a public link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "hJ4IL9deVcKq",
    "outputId": "0f4a69c3-9522-4515-d16e-3ff32eaa8c42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/usr/local/lib/python3.8/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://8addf643791e4b8dea.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8addf643791e4b8dea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a public link\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Define a function to make predictions with your model\n",
    "def predict_category(text):\n",
    "    prediction = inference(model, text)\n",
    "    return prediction\n",
    "\n",
    "# Define a Gradio interface for your function\n",
    "iface = gr.Interface(\n",
    "    fn=predict_category,\n",
    "    inputs=gr.inputs.Textbox(label=\"Enter text here\"),\n",
    "    outputs=gr.outputs.Textbox(label=\"Predicted category\"),\n",
    "    title=\"Text Classification Demo\",\n",
    "    description=\"Enter some text and see which category it belongs to.\",\n",
    ")\n",
    "\n",
    "# Launch the interface on a public server\n",
    "iface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
